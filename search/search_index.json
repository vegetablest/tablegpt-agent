{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>tablegpt-agent is a pre-built agent for TableGPT2 (huggingface), a series of LLMs for table-based question answering. This agent is built on top of the Langgraph library and provides a user-friendly interface for interacting with TableGPT2.</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<ul> <li>Tutorials<ul> <li>Quickstart</li> <li>Chat on Tabular Data</li> <li>Continue Analysis on Generated Charts</li> </ul> </li> <li>How-To Guides<ul> <li>Enhance TableGPT Agent with RAG</li> <li>Persist Messages</li> <li>Incluster Code Execution</li> <li>Normalize Datasets</li> </ul> </li> <li>Explanation<ul> <li>Agent Workflow</li> <li>File Reading</li> </ul> </li> <li>Reference</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Thank you for your interest in TableGPT Agent. For more information on contributing, please see the contributing guide.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We extend our sincere gratitude to all contributors and collaborators who played a pivotal role in the development of tablegpt-agent. Special thanks to our team members and the open-source community, whose insights and feedback were invaluable throughout the project.</p> <p>Thank you to our early users for their suggestions and engagement, which have greatly helped in refining and enhancing this tool.</p>"},{"location":"reference/","title":"API Reference","text":"<p>Creates a state graph for processing datasets.</p> <p>This function orchestrates the creation of a workflow for handling table data. It sets up nodes for reading files and analyzing data based on provided parameters. The graph dynamically routes based on the presence of file attachments in the input state.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Runnable</code> <p>The primary language model for processing user input.</p> required <code>pybox_manager</code> <code>BasePyBoxManager</code> <p>A python code sandbox delegator, used to execute the data analysis code generated by llm.</p> required <code>session_id</code> <code>str | None</code> <p>An optional session identifier used to associate with <code>pybox</code>. Defaults to None.</p> <code>None</code> <code>workdir</code> <code>Path | None</code> <p>The working directory for <code>pybox</code> operations. Defaults to None.</p> <code>None</code> <code>error_trace_cleanup</code> <code>bool</code> <p>Flag to clean up error traces. Defaults to False.</p> <code>False</code> <code>nlines</code> <code>int | None</code> <p>Number of lines to read for preview. Defaults to None.</p> <code>None</code> <code>vlm</code> <code>BaseLanguageModel | None</code> <p>Optional vision language model for processing images. Defaults to None.</p> <code>None</code> <code>safety_llm</code> <code>Runnable | None</code> <p>Model used for safety classification of inputs. Defaults to None.</p> <code>None</code> <code>dataset_retriever</code> <code>BaseRetriever | None</code> <p>Component to retrieve datasets. Defaults to None.</p> <code>None</code> <code>normalize_llm</code> <code>BaseLanguageModel | None</code> <p>Model for data normalization tasks. Defaults to None.</p> <code>None</code> <code>locate</code> <code>str | None</code> <p>The locale of the user. Defaults to None.</p> required <code>checkpointer</code> <code>BaseCheckpointSaver | None</code> <p>Component for saving checkpoints. Defaults to None.</p> <code>None</code> <code>llm_truncation_config</code> <code>TruncationConfig | None</code> <p>Truncation config for LLM. Defaults to None.</p> <code>None</code> <code>vlm_truncation_config</code> <code>TruncationConfig | None</code> <p>Truncation config for VLM. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>CompiledStateGraph</code> <code>CompiledStateGraph</code> <p>A compiled state graph representing the table processing workflow.</p> Source code in <code>src/tablegpt/agent/__init__.py</code> <pre><code>def create_tablegpt_graph(\n    llm: BaseLanguageModel,\n    pybox_manager: BasePyBoxManager,\n    *,\n    session_id: str | None = None,\n    workdir: Path | None = None,\n    error_trace_cleanup: bool = False,\n    nlines: int | None = None,\n    vlm: BaseLanguageModel | None = None,\n    safety_llm: Runnable | None = None,\n    dataset_retriever: BaseRetriever | None = None,\n    normalize_llm: BaseLanguageModel | None = None,\n    locale: str | None = None,\n    checkpointer: BaseCheckpointSaver | None = None,\n    llm_truncation_config: TruncationConfig | None = None,\n    vlm_truncation_config: TruncationConfig | None = None,\n    verbose: bool = False,\n) -&gt; CompiledStateGraph:\n    \"\"\"Creates a state graph for processing datasets.\n\n    This function orchestrates the creation of a workflow for handling table data.\n    It sets up nodes for reading files and analyzing data based on provided parameters.\n    The graph dynamically routes based on the presence of file attachments in the input state.\n\n    Args:\n        llm (Runnable): The primary language model for processing user input.\n        pybox_manager (BasePyBoxManager):  A python code sandbox delegator, used to execute the data analysis code generated by llm.\n        session_id (str | None, optional): An optional session identifier used to associate with `pybox`. Defaults to None.\n        workdir (Path | None, optional): The working directory for `pybox` operations. Defaults to None.\n        error_trace_cleanup (bool, optional): Flag to clean up error traces. Defaults to False.\n        nlines (int | None, optional): Number of lines to read for preview. Defaults to None.\n        vlm (BaseLanguageModel | None, optional): Optional vision language model for processing images. Defaults to None.\n        safety_llm (Runnable | None, optional): Model used for safety classification of inputs. Defaults to None.\n        dataset_retriever (BaseRetriever | None, optional): Component to retrieve datasets. Defaults to None.\n        normalize_llm (BaseLanguageModel | None, optional): Model for data normalization tasks. Defaults to None.\n        locate (str | None, optional): The locale of the user. Defaults to None.\n        checkpointer (BaseCheckpointSaver | None, optional): Component for saving checkpoints. Defaults to None.\n        llm_truncation_config (TruncationConfig | None, optional): Truncation config for LLM. Defaults to None.\n        vlm_truncation_config (TruncationConfig | None, optional): Truncation config for VLM. Defaults to None.\n        verbose (bool, optional): Flag to enable verbose logging. Defaults to False.\n\n    Returns:\n        CompiledStateGraph: A compiled state graph representing the table processing workflow.\n    \"\"\"\n    workflow = StateGraph(AgentState)\n    file_reading_graph = create_file_reading_workflow(\n        nlines=nlines,\n        llm=llm,\n        pybox_manager=pybox_manager,\n        workdir=workdir,\n        session_id=session_id,\n        normalize_llm=normalize_llm,\n        locale=locale,\n        verbose=verbose,\n    )\n    data_analyze_graph = create_data_analyze_workflow(\n        llm=llm,\n        pybox_manager=pybox_manager,\n        workdir=workdir,\n        session_id=session_id,\n        error_trace_cleanup=error_trace_cleanup,\n        vlm=vlm,\n        safety_llm=safety_llm,\n        dataset_retriever=dataset_retriever,\n        llm_truncation_config=llm_truncation_config,\n        vlm_truncation_config=vlm_truncation_config,\n        verbose=verbose,\n    )\n\n    def router(state: AgentState) -&gt; str:\n        # Must have at least one message when entering this router\n        last_message = state[\"messages\"][-1]\n        if last_message.additional_kwargs.get(\"attachments\"):\n            return \"file_reading_graph\"\n        return \"data_analyze_graph\"\n\n    workflow.add_node(\"file_reading_graph\", file_reading_graph)\n    workflow.add_node(\"data_analyze_graph\", data_analyze_graph)\n\n    workflow.add_conditional_edges(START, router)\n    workflow.add_edge(\"file_reading_graph\", END)\n    workflow.add_edge(\"data_analyze_graph\", END)\n\n    return workflow.compile(checkpointer=checkpointer, debug=verbose)\n</code></pre>"},{"location":"explanation/agent-workflow/","title":"Agent Workflow","text":"<p>The Agent Workflow is the core functionality of the <code>tablegpt-agent</code>. It processes user input and generates appropriate responses. This workflow is similar to those found in most single-agent systems and consists of an agent and various tools. Specifically, the data analysis workflow includes:</p> <ul> <li>An Agent Powered by TableGPT2: This agent performs data analysis tasks. It is designed to understand and execute complex data analysis queries, providing accurate and insightful results.</li> <li>An IPython tool: This tool executes the generated code within a sandbox environment, ensuring that the code runs safely and efficiently.</li> </ul> <p>Additionally, TableGPT Agent offers several optional plugins that extend the agent's functionality:</p> <ul> <li>Visual Language Model: This plugin can be used to enhance summarization for data visualization tasks.</li> <li>Retriever: This plugin fetches information about the dataset, improving the quality and relevance of the generated code.</li> <li>Safety Mechanism: This plugin protects the system from toxic inputs.</li> </ul>"},{"location":"explanation/agent-workflow/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>User Input: The user provides a query or command to the agent.</li> <li>Security Assessment (optional): The agent evaluates whether the user's query involves sensitive topics. If it does, the agent will prompt the LLM to be cautious in its response.</li> <li>Data Retrieval(optional): The retriever plugin fetches relevant data and metadata.</li> <li>Code Generation: The agent generates the appropriate code to perform the requested task.</li> <li>Code Execution: The generated code is executed in the IPython sandbox environment.</li> <li>Result Generation: The agent processes the results of the code execution and generates a response.</li> <li>Visual analysis (optional): Perform visual analysis and summary on the generated pictures to enrich the answers.</li> </ol> <p>NOTE: During the operation of <code>tablegpt-agent</code>, the system will repeatedly attempt to resolve any issues that arise during code execution. As a result, Steps <code>4~7</code> may be executed multiple times in an iterative debugging process. This cycle continues until the final solution is reached or the maximum iteration limit is exceeded. The default maximum iteration count is 25.</p>"},{"location":"explanation/agent-workflow/#example-workflow","title":"Example Workflow","text":""},{"location":"explanation/agent-workflow/#user-input","title":"User Input","text":"<p>The user inputs a query:</p> <p>Draw a pie chart of the male-female ratio based on gender.</p>"},{"location":"explanation/agent-workflow/#security-assessment-optional","title":"Security Assessment (Optional)","text":"<p>When creating a <code>tablegpt-agent</code>, you can specify a dedicated safety evaluation model using the <code>safety_llm</code> parameter. For example:</p> <pre><code>from langchain_openai import ChatOpenAI\nfrom tablegpt.agent import create_tablegpt_graph\n\nsafety_llm = ChatOpenAI(openai_api_base=\"LLM_URL\", openai_api_key=\"whatever\", model_name=\"LLM_MODEL_NAME\")\n\nagent = create_tablegpt_graph(\n    ...\n    safety_llm=safety_llm,\n    ...\n)\n</code></pre> <p>If <code>safety_llm</code> is configured, it will evaluate user inputs to identify if they fall under any of the following <code>14</code> hazard categories. If a question is flagged as belonging to one of these categories, <code>tablegpt-agent</code> will respond with caution.</p> Hazard Category Description S1: Violent Crimes Violent criminal activities S2: Non-Violent Crimes Non-violent criminal activities S3: Sex-Related Crimes Crimes related to sexual misconduct S4: Child Sexual Exploitation Exploitation of children S5: Defamation Defamatory content S6: Specialized Advice Professional advice (e.g., medical, legal) S7: Privacy Privacy violations S8: Intellectual Property Intellectual property issues S9: Indiscriminate Weapons Use or production of indiscriminate weapons S10: Hate Hateful or discriminatory content S11: Suicide &amp; Self-Harm Suicide or self-harm-related content S12: Sexual Content Explicit sexual content S13: Elections Content related to elections S14: Code Interpreter Abuse Misuse of code interpretation features <p>This feature enhances the safety of the <code>tablegpt-agent</code>, helping to mitigate ethical and legal risks associated with generated content.</p>"},{"location":"explanation/agent-workflow/#data-retrieval-optional","title":"Data Retrieval (optional)","text":"<p>The retriever plugin recalls columns and values related to the query, enhancing the LLM's understanding of the dataset. This improves the accuracy of the code generated by the LLM. For detailed usage instructions, refer to Enhance TableGPT Agent with RAG.</p> <p>For this example, based on the user\u2019s input, the retrieved results are as follows:</p> <pre><code>Here are some extra column information that might help you understand the dataset:\\n- titanic.csv:\\n  - {\"column\": Sex, \"dtype\": \"string\", \"values\": [\"male\", \"female\", ...]}\n</code></pre>"},{"location":"explanation/agent-workflow/#code-generation","title":"Code Generation","text":"<p>The agent generates the following Python code: <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Count the number of males and females\ngender_counts = df1['Sex'].value_counts()\n\n# Create a pie chart\nplt.figure(figsize=(6, 6))\nplt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=140)\nplt.title('Gender Distribution')\nplt.show()\n</code></pre></p>"},{"location":"explanation/agent-workflow/#code-execution","title":"Code Execution","text":"<p>The generated code is automatically executed in the IPython sandbox environment.</p>"},{"location":"explanation/agent-workflow/#result-generation","title":"Result Generation","text":"<p>After the execution is complete, the results are generated as follows:</p> <p></p>"},{"location":"explanation/agent-workflow/#visual-analysis-optional","title":"Visual Analysis (optional)","text":"<p>The visual analysis plugin allows you to enhance generated results with visualizations, making the output more intuitive and informative.</p> <p>To enable this feature, you can pass the <code>vlm</code> parameter when creating a <code>tablegpt-agent</code>. Here\u2019s an example:</p> <pre><code>from langchain_openai import ChatOpenAI\nfrom tablegpt.agent import create_tablegpt_graph\n\nvlm = ChatOpenAI(openai_api_base=\"VLM_URL\", openai_api_key=\"whatever\", model_name=\"VLM_MODEL_NAME\")\n\nagent = create_tablegpt_graph(\n    ...\n    vlm=vlm,\n    ...\n)\n</code></pre> <p>Once enabled, the <code>tablegpt-agent</code> will use the <code>vlm</code> model to generate visual representations of the data.  </p> <p>For instance, in response to the query mentioned earlier, the <code>tablegpt-agent</code> generates the following visualization:</p> <p>I have drawn a pie chart illustrating the ratio of men to women. From the chart, you can see that men constitute 64.4% while women make up 35.6%. If you need any further analysis or visualizations, feel free to let me know. </p> <p>This feature adds a layer of clarity and insight, helping users interpret the results more effectively. On some complex graphs, this function is more effective.</p>"},{"location":"explanation/code-sandbox/","title":"Code Sandbox","text":"<p><code>tablegpt-agent</code> directs <code>tablegpt</code> to generate Python code for data analysis. However, the generated code may contain potential vulnerabilities or unexpected errors. Running such code directly in a production environment could threaten the system's stability and security.</p> <p><code>Code Sandbox</code> is designed to address this challenge. By leveraging sandbox technology, it confines code execution to a controlled environment, effectively preventing malicious or unexpected behaviors from impacting the main system. This provides an isolated and reliable space for running code safely.</p> <p><code>Code Sandbox</code> built on the pybox library and supports three main execution modes:</p> <ul> <li>Local Environment: Executes code in a local sandbox for quick deployment and validation.  </li> <li>Remote Environment: Create remote environments through <code>Jupyter Enterprise Gateway</code> to achieve shared computing.</li> <li>Cluster Environment: Bypassing the need for proxy services such as <code>Jupyter Enterprise Gateway</code> by communicating directly with kernel pods.</li> </ul> <p>Code Sandbox is designed based on the following key principles:</p> <ul> <li>Security: Limits code access using sandbox technology to ensure a safe and reliable execution environment.  </li> <li>Isolation: Provides independent execution environments for each task, ensuring strict separation of resources and data.  </li> <li>Scalability: Adapts to diverse computing environments, from local setups to Kubernetes clusters, supporting dynamic resource allocation and efficient task execution.</li> </ul>"},{"location":"explanation/code-sandbox/#local-environment","title":"Local Environment","text":"<p>In a local environment, Code Sandbox utilizes the <code>pybox</code> library to create and manage sandbox environments, providing a secure code execution platform. By isolating code execution from the host system's resources and imposing strict permission controls, it ensures safety and reliability. This approach is especially suitable for development and debugging scenarios.</p> <p>If you want to run <code>tablegpt-agent</code> in a local environment, you can enable the local mode. Below are the installation steps and a detailed operation guide.</p>"},{"location":"explanation/code-sandbox/#installing","title":"Installing","text":"<p>To use <code>tablegpt-agent</code> in local mode, install the library with the following command:</p> <pre><code>pip install tablegpt-agent[local]\n</code></pre>"},{"location":"explanation/code-sandbox/#configuring","title":"Configuring","text":"<p><code>tablegpt-agent</code> comes with several built-in features, such as auxiliary methods for data analysis and setting display font. These features are automatically added to the sandbox environment by default. If you need advanced customization (e.g., adding specific methods or fonts), refer to the TableGPT IPython Kernel Configuration Documentation for further guidance.</p>"},{"location":"explanation/code-sandbox/#creating-and-running","title":"Creating and Running","text":"<p>The following code demonstrates how to use the pybox library to set up a sandbox, execute code, and retrieve results in a local environment:</p> <pre><code>from uuid import uuid4\nfrom pybox import LocalPyBoxManager, PyBoxOut\n\n# Initialize the local sandbox manager\npybox_manager = LocalPyBoxManager()\n\n# Assign a unique Kernel ID for the sandbox\nkernel_id = str(uuid4())\n\n# Start the sandbox environment\nbox = pybox_manager.start(kernel_id)\n\n# Define the test code to execute\ntest_code = \"\"\"\nimport math\nresult = math.sqrt(16)\nresult\n\"\"\"\n\n# Run the code in the sandbox\nout: PyBoxOut = box.run(code=test_code)\n\n# Print the execution result\nprint(out)\n</code></pre>"},{"location":"explanation/code-sandbox/#example-output","title":"Example Output","text":"<p>After running the above code, the system will return the following output, indicating successful execution with no errors: <pre><code>data=[{'text/plain': '4.0'}] error=None\n</code></pre></p> <p>With <code>Code Sandbox</code> in local execution mode, developers can enjoy the safety of sandbox isolation at minimal cost while maintaining flexibility and efficiency. This lays a solid foundation for more complex remote or cluster-based scenarios.</p>"},{"location":"explanation/code-sandbox/#remote-environment","title":"Remote Environment","text":"<p>In a remote environment, <code>Code Sandbox</code> uses the <code>pybox</code> library and its <code>RemotePyBoxManager</code> to create and manage sandbox environments. The remote mode relies on the Enterprise Gateway service to dynamically create and execute remote sandboxes. This mode allows multiple services to connect to the same remote environment, enabling shared access to resources. </p>"},{"location":"explanation/code-sandbox/#configuring_1","title":"Configuring","text":"<p>If <code>tablegpt-agent</code> is used in remote mode, the first step is to start the <code>enterprise_gateway</code> service. You can refer to the Enterprise Gateway Deployment Guide for detailed instructions on configuring and starting the service.</p> <p>Once the service is up and running, ensure that the service address is accessible. For example, assume the <code>enterprise_gateway</code> service is available at <code>http://example.com</code>.</p>"},{"location":"explanation/code-sandbox/#creating-and-running_1","title":"Creating and Running","text":"<p>The following code demonstrates how to create a remote sandbox using <code>RemotePyBoxManager</code> and execute code within it:</p> <pre><code>from uuid import uuid4\nfrom pybox import RemotePyBoxManager, PyBoxOut\n\n# Initialize the remote sandbox manager, replacing with the actual Enterprise Gateway service address\npybox_manager = RemotePyBoxManager(host=\"http://example.com\")\n\n# Assign a unique Kernel ID\nkernel_id = str(uuid4())\n\n# Start the remote sandbox environment\nbox = pybox_manager.start(kernel_id)\n\n# Define the test code\ntest_code = \"\"\"\nimport math\nresult = math.sqrt(16)\nresult\n\"\"\"\n\n# Run the code in the sandbox\nout: PyBoxOut = box.run(code=test_code)\n\n# Print the execution result\nprint(out)\n</code></pre>"},{"location":"explanation/code-sandbox/#example-output_1","title":"Example Output","text":"<p>After executing the above code, the system will return the following output, indicating successful execution without any errors:</p> <pre><code>data=[{'text/plain': '4.0'}] error=None\n</code></pre>"},{"location":"explanation/code-sandbox/#advanced-environment-configuration","title":"Advanced Environment Configuration","text":"<p>The <code>RemotePyBoxManager</code> provides the following advanced configuration options to allow for flexible customization of the sandbox execution environment:  </p> <ol> <li><code>env_file</code>: Allows you to load environment variables from a file to configure the remote sandbox.  </li> <li><code>kernel_env</code>: Enables you to pass environment variables directly as key-value pairs, simplifying the setup process.  </li> </ol> <p>To learn more about the parameters and configuration options, refer to the Kernel Environment Variables documentation.</p>"},{"location":"explanation/code-sandbox/#cluster-environment","title":"Cluster Environment","text":"<p>In a Kubernetes cluster, <code>Code Sandbox</code> leverages the <code>KubePyBoxManager</code> provided by the <code>pybox</code> library to create and manage sandboxes. Unlike the <code>remote environment</code>, the cluster environment communicates directly with Kernel Pods created by the Jupyter Kernel Controller, eliminating the need for an intermediary service like <code>Enterprise Gateway</code>.</p>"},{"location":"explanation/code-sandbox/#configuring_2","title":"Configuring","text":"<p>Before using the cluster environment, you need to deploy the <code>jupyter-kernel-controller</code> service. You can quickly create the required CRDs and Deployments using the Deploy Documentation.  </p>"},{"location":"explanation/code-sandbox/#creating-and-running_2","title":"Creating and Running","text":"<p>Once the <code>jupyter-kernel-controller</code> service is successfully deployed and running, you can create and run a cluster sandbox using the following code:  </p> <pre><code>from uuid import uuid4\nfrom pybox import KubePyBoxManager, PyBoxOut\n\n# Initialize the cluster sandbox manager, replacing with actual paths and environment variable configurations\npybox_manager = KubePyBoxManager(\n    env_file=\"YOUR_ENV_FILE_PATH\",  # Path to the environment variable file\n    kernel_env=\"YOUR_KERNEL_ENV_DICT\",  # Kernel environment variable configuration\n)\n\n# Assign a unique Kernel ID\nkernel_id = str(uuid4())\n\n# Start the cluster sandbox environment\nbox = pybox_manager.start(kernel_id)\n\n# Define the test code\ntest_code = \"\"\"\nimport math\nresult = math.sqrt(16)\nresult\n\"\"\"\n\n# Run the code in the sandbox\nout: PyBoxOut = box.run(code=test_code)\n\n# Print the execution result\nprint(out)\n</code></pre>"},{"location":"explanation/code-sandbox/#example-output_2","title":"Example Output","text":"<p>After executing the code above, the following output will be returned, indicating successful execution without any errors:  </p> <pre><code>data=[{'text/plain': '4.0'}] error=None\n</code></pre> <p>NOTE: The <code>env_file</code> and <code>kernel_env</code> parameters required by <code>KubePyBoxManager</code> are essentially the same as those for <code>RemotePyBoxManager</code>. For detailed information about these parameters, please refer to the RemotePyBoxManager Advanced Environment Configuration.</p> <p>With the above configuration, you can efficiently manage secure and reliable sandboxes in a Kubernetes cluster, supporting flexible control and extension of execution results.</p>"},{"location":"explanation/file-reading/","title":"File Reading","text":"<p>Here's how the workflow unfolds:</p> In\u00a0[1]: Copied! <pre># Load the data into a DataFrame\ndf1 = read_df('\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx', header=[0, 1, 2])\ndf1.head(5)\n</pre> # Load the data into a DataFrame df1 = read_df('\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx', header=[0, 1, 2]) df1.head(5) Out[1]: \u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868 \u751f\u4ea7\u65e5\u671f \u5236\u9020\u7f16\u53f7 \u4ea7\u54c1\u540d\u79f0 \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf \u7d2f\u8ba1\u4ea7\u91cf \u8017\u8d39\u5de5\u65f6 Unnamed: 0_level_2 Unnamed: 1_level_2 Unnamed: 2_level_2 Unnamed: 3_level_2 \u9884\u8ba1 \u5b9e\u9645 Unnamed: 6_level_2 \u672c\u65e5 \u7d2f\u8ba1 0 2007-08-10 00:00:00 FK-001 \u7315\u7334\u6843\u679c\u8089\u996e\u6599 100000.0 40000 45000 83000 10.0 20.0 1 2007-08-11 00:00:00 FK-002 \u897f\u74dc\u679c\u8089\u996e\u6599 100000.0 40000 44000 82000 9.0 18.0 2 2007-08-12 00:00:00 FK-003 \u8349\u8393\u679c\u8089\u996e\u6599 100000.0 40000 45000 83000 9.0 18.0 3 2007-08-13 00:00:00 FK-004 \u84dd\u8393\u679c\u8089\u996e\u6599 100000.0 40000 45000 83000 9.0 18.0 4 2007-08-14 00:00:00 FK-005 \u6c34\u5bc6\u6843\u679c\u8089\u996e\u6599 100000.0 40000 45000 83000 10.0 20.0 <p>The file is riddled with merged cells, empty rows, and redundant formatting that make it incompatible with pandas. If you try to load this file directly, pandas might misinterpret the structure or fail to parse it entirely.</p> <p>With our normalization feature, irregular datasets can be seamlessly transformed into clean, structured formats. When using the <code>create_tablegpt_agent</code> method, simply pass the <code>normalize_llm</code> parameter. The system will automatically analyze the irregular data and generate the appropriate transformation code, ensuring the dataset is prepared in the optimal format for further analysis.</p> <p>Below is an example of the code generated for the provided irregular dataset:</p> In\u00a0[2]: Copied! <pre># Normalize the data\ntry:\n    df = df1.copy()\n\n    import pandas as pd\n\n    # Assuming the original data is loaded into a DataFrame named df\n    # Here is the transformation process:\n\n    # Step 1: Isolate the Table Header\n    # Remove the unnecessary top rows and columns\n    final_df = df.iloc[2:, :9].copy()\n\n    # Step 2: Rename Columns of final_df\n    # Adjust the column names to match the desired format\n    final_df.columns = ['\u751f\u4ea7\u65e5\u671f', '\u5236\u9020\u7f16\u53f7', '\u4ea7\u54c1\u540d\u79f0', '\u9884\u5b9a\u4ea7\u91cf', '\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1', '\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645', '\u7d2f\u8ba1\u4ea7\u91cf', '\u672c\u65e5\u8017\u8d39\u5de5\u65f6', '\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6']\n\n    # Step 3: Data Processing\n    # Ensure there are no NaN values and drop any duplicate rows if necessary\n    final_df.dropna(inplace=True)\n    final_df.drop_duplicates(inplace=True)\n\n    # Convert the appropriate columns to numeric types\n    final_df['\u9884\u5b9a\u4ea7\u91cf'] = final_df['\u9884\u5b9a\u4ea7\u91cf'].astype(int)\n    final_df['\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1'] = final_df['\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1'].astype(int)\n    final_df['\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645'] = final_df['\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645'].astype(int)\n    final_df['\u7d2f\u8ba1\u4ea7\u91cf'] = final_df['\u7d2f\u8ba1\u4ea7\u91cf'].astype(int)\n    final_df['\u672c\u65e5\u8017\u8d39\u5de5\u65f6'] = final_df['\u672c\u65e5\u8017\u8d39\u5de5\u65f6'].astype(int)\n    final_df['\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6'] = final_df['\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6'].astype(int)\n\n    # Display the transformed DataFrame\n    if final_df.columns.tolist() == final_df.iloc[0].tolist():\n        final_df = final_df.iloc[1:]\n\n    # reassign df1 with the formatted DataFrame\n    df1 = final_df\nexcept Exception as e:\n    # Unable to apply formatting to the original DataFrame. proceeding with the unformatted DataFrame.\n    print(f\"Reformat failed with error {e}, use the original DataFrame.\")\n</pre> # Normalize the data try:     df = df1.copy()      import pandas as pd      # Assuming the original data is loaded into a DataFrame named df     # Here is the transformation process:      # Step 1: Isolate the Table Header     # Remove the unnecessary top rows and columns     final_df = df.iloc[2:, :9].copy()      # Step 2: Rename Columns of final_df     # Adjust the column names to match the desired format     final_df.columns = ['\u751f\u4ea7\u65e5\u671f', '\u5236\u9020\u7f16\u53f7', '\u4ea7\u54c1\u540d\u79f0', '\u9884\u5b9a\u4ea7\u91cf', '\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1', '\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645', '\u7d2f\u8ba1\u4ea7\u91cf', '\u672c\u65e5\u8017\u8d39\u5de5\u65f6', '\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6']      # Step 3: Data Processing     # Ensure there are no NaN values and drop any duplicate rows if necessary     final_df.dropna(inplace=True)     final_df.drop_duplicates(inplace=True)      # Convert the appropriate columns to numeric types     final_df['\u9884\u5b9a\u4ea7\u91cf'] = final_df['\u9884\u5b9a\u4ea7\u91cf'].astype(int)     final_df['\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1'] = final_df['\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1'].astype(int)     final_df['\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645'] = final_df['\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645'].astype(int)     final_df['\u7d2f\u8ba1\u4ea7\u91cf'] = final_df['\u7d2f\u8ba1\u4ea7\u91cf'].astype(int)     final_df['\u672c\u65e5\u8017\u8d39\u5de5\u65f6'] = final_df['\u672c\u65e5\u8017\u8d39\u5de5\u65f6'].astype(int)     final_df['\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6'] = final_df['\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6'].astype(int)      # Display the transformed DataFrame     if final_df.columns.tolist() == final_df.iloc[0].tolist():         final_df = final_df.iloc[1:]      # reassign df1 with the formatted DataFrame     df1 = final_df except Exception as e:     # Unable to apply formatting to the original DataFrame. proceeding with the unformatted DataFrame.     print(f\"Reformat failed with error {e}, use the original DataFrame.\") <p>Using the generated transformation code, the irregular dataset is converted into a clean, structured format, ready for analysis:</p> In\u00a0[3]: Copied! <pre>df1.head(5)\n</pre> df1.head(5) Out[3]: \u751f\u4ea7\u65e5\u671f \u5236\u9020\u7f16\u53f7 \u4ea7\u54c1\u540d\u79f0 \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1 \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645 \u7d2f\u8ba1\u4ea7\u91cf \u672c\u65e5\u8017\u8d39\u5de5\u65f6 \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6 2 2007-08-12 00:00:00 FK-003 \u8349\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 3 2007-08-13 00:00:00 FK-004 \u84dd\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 4 2007-08-14 00:00:00 FK-005 \u6c34\u5bc6\u6843\u679c\u8089\u996e\u6599 100000 40000 45000 83000 10 20 5 2007-08-15 00:00:00 FK-006 \u8354\u679d\u679c\u8089\u996e\u6599 100000 40000 44000 82000 10 20 6 2007-08-16 00:00:00 FK-007 \u6a31\u6843\u679c\u8089\u996e\u6599 100000 40000 46000 84000 9 18 In\u00a0[4]: Copied! <pre># Remove leading and trailing whitespaces in column names\ndf1.columns = df1.columns.str.strip()\n\n# Remove rows and columns that contain only empty values\ndf1 = df1.dropna(how='all').dropna(axis=1, how='all')\n\n# Get the basic information of the dataset\ndf1.info(memory_usage=False)\n</pre> # Remove leading and trailing whitespaces in column names df1.columns = df1.columns.str.strip()  # Remove rows and columns that contain only empty values df1 = df1.dropna(how='all').dropna(axis=1, how='all')  # Get the basic information of the dataset df1.info(memory_usage=False) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 18 entries, 2 to 19\nData columns (total 9 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   \u751f\u4ea7\u65e5\u671f    18 non-null     object\n 1   \u5236\u9020\u7f16\u53f7    18 non-null     object\n 2   \u4ea7\u54c1\u540d\u79f0    18 non-null     object\n 3   \u9884\u5b9a\u4ea7\u91cf    18 non-null     int64 \n 4   \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1  18 non-null     int64 \n 5   \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645  18 non-null     int64 \n 6   \u7d2f\u8ba1\u4ea7\u91cf    18 non-null     int64 \n 7   \u672c\u65e5\u8017\u8d39\u5de5\u65f6  18 non-null     int64 \n 8   \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6  18 non-null     int64 \ndtypes: int64(6), object(3)</pre> In\u00a0[5]: Copied! <pre># Show the first 5 rows to understand the structure\ndf1.head(5)\n</pre> # Show the first 5 rows to understand the structure df1.head(5) Out[5]: \u751f\u4ea7\u65e5\u671f \u5236\u9020\u7f16\u53f7 \u4ea7\u54c1\u540d\u79f0 \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1 \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645 \u7d2f\u8ba1\u4ea7\u91cf \u672c\u65e5\u8017\u8d39\u5de5\u65f6 \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6 2 2007-08-12 00:00:00 FK-003 \u8349\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 3 2007-08-13 00:00:00 FK-004 \u84dd\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 4 2007-08-14 00:00:00 FK-005 \u6c34\u5bc6\u6843\u679c\u8089\u996e\u6599 100000 40000 45000 83000 10 20 5 2007-08-15 00:00:00 FK-006 \u8354\u679d\u679c\u8089\u996e\u6599 100000 40000 44000 82000 10 20 6 2007-08-16 00:00:00 FK-007 \u6a31\u6843\u679c\u8089\u996e\u6599 100000 40000 46000 84000 9 18"},{"location":"explanation/file-reading/#file-reading","title":"File Reading\u00b6","text":"<p>When working with dataset files, maintaining a clear separation between file reading and data analysis workflows can significantly improve control and clarity. At TableGPT Agent, we've designed a robust and structured approach to handling file reading that empowers the LLM (Large Language Model) to effectively analyze dataset files without being overwhelmed by unnecessary details. This method not only enhances the LLM's ability to inspect the data but also ensures a smoother and more reliable data analysis process.</p> <p>Traditionally, allowing an LLM to directly inspect a dataset might involve simply calling the <code>df.head()</code> function to preview its content. While this approach suffices for straightforward use cases, it often lacks depth when dealing with more complex or messy datasets. To address this, we've developed a multi-step file reading workflow designed to deliver richer insights into the dataset structure while preparing it for advanced analysis.</p>"},{"location":"explanation/file-reading/#normalization-optional","title":"Normalization (Optional)\u00b6","text":"<p>Not all files are immediately suitable for direct analysis. Excel files, in particular, can pose challenges\u2014irregular formatting, merged cells, and inconsistent headers are just a few examples. To tackle these issues, we introduce an optional normalization step that preprocesses the data, transforming it into a format that is \u201cpandas-friendly.\u201d</p> <p>This step addresses the most common quirks in Excel files, such as non-standard column headers, inconsistent row structures, or missing metadata. By resolving these typical issues upfront, the data is transformed into a format that is 'pandas-friendly' ensuring smooth integration with downstream processes.</p> <p>Example Scenario:</p> <p>Imagine you have an Excel file that looks like this:</p>"},{"location":"explanation/file-reading/#dataset-structure-overview","title":"Dataset Structure Overview\u00b6","text":"<p>After normalization, the next step dives into the structural aspects of the dataset using the <code>df.info()</code> function. Unlike <code>df.head()</code>, which only shows a snippet of the data, <code>df.info()</code> provides a holistic view of the dataset\u2019s structure. Key insights include:</p> <ul> <li>Column Data Types: Helps identify numerical, categorical, or textual data at a glance.</li> <li>Non-Null Counts: Reveals the completeness of each column, making it easy to spot potential gaps or inconsistencies.</li> </ul> <p>By focusing on the foundational structure of the dataset, this step enables the LLM to better understand the quality and layout of the data, paving the way for more informed analyses.</p>"},{"location":"explanation/file-reading/#dataset-content-preview","title":"Dataset Content Preview\u00b6","text":"<p>Finally, we utilize the <code>df.head()</code> function to provide a visual preview of the dataset\u2019s content. This step is crucial for understanding the actual values within the dataset\u2014patterns, anomalies, or trends often become apparent here.</p> <p>The number of rows displayed (<code>n</code>) is configurable to balance between granularity and simplicity. For smaller datasets or detailed exploration, a larger <code>n</code> might be beneficial. However, for larger datasets, displaying too many rows could overwhelm the LLM with excessive details, detracting from the primary analytical objectives.</p>"},{"location":"explanation/file-reading/#why-this-matters","title":"Why This Matters\u00b6","text":"<p>This structured, multi-step approach is not just about processing data; it's about making the LLM smarter in how it interacts with datasets. By systematically addressing issues like messy formatting, structural ambiguity, and information overload, we ensure the LLM operates with clarity and purpose.</p> <p>The separation of file reading from analysis offers several advantages:</p> <ul> <li>Enhanced Accuracy: Preprocessing and structure-checking reduce the risk of errors in downstream analyses.</li> <li>Scalability: Handles datasets of varying complexity and size with equal efficiency.</li> <li>Transparency: Provides clear visibility into the dataset\u2019s structure, enabling better decision-making.</li> </ul> <p>By adopting this method, TableGPT Agent transforms the way dataset files are read and analyzed, offering a smarter, more controlled, and ultimately more user-friendly experience.</p>"},{"location":"explanation/ipython-startup-scripts/","title":"IPython Startup Scripts","text":""},{"location":"howto/cleanup-error-trace/","title":"Cleanup Error Trace","text":""},{"location":"howto/customize-table-info/","title":"Customize Table Info","text":""},{"location":"howto/incluster-code-execution/","title":"Incluster Code Execution","text":"<p>The <code>tablegpt-agent</code> directs <code>tablegpt</code> to generate Python code for data analysis. This code is then executed within a sandbox environment to ensure system security. The execution is managed by the pybox library, which provides a simple way to run Python code outside the main process.</p>"},{"location":"howto/messages-truncation/","title":"Messages Truncation","text":"In\u00a0[1]: Copied! <pre>from datetime import date\n\nfrom langchain_core.messages import HumanMessage,AIMessage,SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom tablegpt.agent import create_tablegpt_graph\nfrom pybox import AsyncLocalPyBoxManager\npybox_manager = AsyncLocalPyBoxManager()\n</pre> from datetime import date  from langchain_core.messages import HumanMessage,AIMessage,SystemMessage from langchain_openai import ChatOpenAI from tablegpt.agent import create_tablegpt_graph from pybox import AsyncLocalPyBoxManager pybox_manager = AsyncLocalPyBoxManager()  In\u00a0[2]: Copied! <pre># Assuming the model service supports max_model_len=1024, which means input_tokens + max_completion_tokens &lt;= 1024\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\",max_tokens=256)\n</pre> # Assuming the model service supports max_model_len=1024, which means input_tokens + max_completion_tokens &lt;= 1024 llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\",max_tokens=256) In\u00a0[3]: Copied! <pre>agent_without_truncation = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager\n)\n</pre> agent_without_truncation = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager ) In\u00a0[4]: Copied! <pre>messages = [\n    SystemMessage(content=\"\u4f60\u662f\u4e00\u4e2a\u53cb\u597d\u7684AI\u52a9\u624b\"),\n    HumanMessage(content=\"\u4f60\u80fd\u7ed9\u6211\u8bb2\u4e00\u4e2a\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6545\u4e8b\u5417\uff1f\"),\n    AIMessage(content=\"\u5f53\u7136\u53ef\u4ee5\u3002\u8ba9\u6211\u4eec\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8d77\u6e90\u5f00\u59cb\u8bb2\u8d77\u3002\u4e00\u5207\u8981\u4ece2017\u5e74\u8c37\u6b4c\u63d0\u51fa\u7684Transformer\u67b6\u6784\u8bf4\u8d77\u3002\u8fd9\u4e2a\u521b\u65b0\u6027\u7684\u67b6\u6784\u4e3a\u540e\u6765\u7684GPT\u3001BERT\u7b49\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002Transformer\u67b6\u6784\u5f15\u5165\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3002\u8fd9\u4e00\u7a81\u7834\u6027\u8fdb\u5c55\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u6587\u672c\u7684\u4e0a\u4e0b\u6587\u8bed\u5883\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u5e26\u6765\u4e86\u9769\u547d\u6027\u7684\u53d8\u5316\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0cOpenAI\u4e8e2018\u5e74\u53d1\u5e03\u4e86\u7b2c\u4e00\u4ee3GPT\u6a21\u578b\uff0c\u968f\u540e\u53c8\u76f8\u7ee7\u63a8\u51fa\u4e86GPT-2\u548cGPT-3\uff0c\u6bcf\u4e00\u4ee3\u90fd\u5728\u89c4\u6a21\u548c\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002\u540c\u65f6\uff0c\u8c37\u6b4c\u63a8\u51fa\u7684BERT\u6a21\u578b\u91c7\u7528\u4e86\u53cc\u5411\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5728\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002\u8fd9\u4e9b\u6a21\u578b\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u66f4\u591a\u7814\u7a76\u8005\u548c\u673a\u6784\u6295\u5165\u5230\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7814\u53d1\u4e2d\uff0c\u63a8\u52a8\u4e86\u6574\u4e2a\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\u3002\u73b0\u5728\uff0c\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u50cfGPT-4\u8fd9\u6837\u7684\u6a21\u578b\u5c55\u73b0\u51fa\u4ee4\u4eba\u60ca\u53f9\u7684\u80fd\u529b\uff0c\u4e0d\u4ec5\u80fd\u591f\u8fdb\u884c\u57fa\u7840\u7684\u6587\u672c\u751f\u6210\uff0c\u8fd8\u80fd\u591f\u7406\u89e3\u4e0a\u4e0b\u6587\u3001\u8fdb\u884c\u63a8\u7406\u3001\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u751a\u81f3\u5c55\u73b0\u51fa\u4e00\u5b9a\u7a0b\u5ea6\u7684\u521b\u9020\u529b...\"),\n    \n    HumanMessage(content=\"\u90a3AI\u662f\u5982\u4f55\u5b66\u4e60\u7406\u89e3\u4eba\u7c7b\u8bed\u8a00\u7684\u5462\uff1f\"),\n    AIMessage(content=\"\u8fd9\u662f\u4e2a\u5f88\u597d\u7684\u95ee\u9898\u3002AI\u901a\u8fc7\u5927\u91cf\u7684\u6587\u672c\u6570\u636e\u8bad\u7ec3\u6765\u7406\u89e3\u8bed\u8a00\u3002\u5b83\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u6349\u8bcd\u8bed\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u4e2a\u9636\u6bb5\uff0c\u9010\u6b65\u638c\u63e1\u8bed\u8a00\u7684\u89c4\u5f8b\u3002\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u6a21\u578b\u4f1a\u9605\u8bfb\u6d77\u91cf\u7684\u6587\u672c\uff0c\u5b66\u4e60\u8bed\u8a00\u7684\u57fa\u672c\u6a21\u5f0f\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u50cf\u4e00\u4e2a\u5a74\u513f\u901a\u8fc7\u89c2\u5bdf\u548c\u6a21\u4eff\u6765\u5b66\u4e60\u8bed\u8a00\u4e00\u6837\u3002\u6a21\u578b\u4f1a\u5206\u6790\u6570\u5341\u4ebf\u751a\u81f3\u6570\u5343\u4ebf\u4e2a\u8bcd\u8bed\uff0c\u7406\u89e3\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u8054\u548c\u4f7f\u7528\u89c4\u5f8b\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u4f1a\u5efa\u7acb\u8d77\u4e00\u4e2a\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u6bcf\u4e2a\u795e\u7ecf\u5143\u90fd\u8d1f\u8d23\u6355\u6349\u7279\u5b9a\u7684\u8bed\u8a00\u7279\u5f81\u3002\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff0c\u6a21\u578b\u4e0d\u65ad\u8c03\u6574\u5176\u5185\u90e8\u53c2\u6570\uff0c\u4ee5\u66f4\u597d\u5730\u9884\u6d4b\u548c\u7406\u89e3\u8bed\u8a00\u3002\u5728\u5fae\u8c03\u9636\u6bb5\uff0c\u6a21\u578b\u4f1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3\uff0c\u6bd4\u5982\u95ee\u7b54\u3001\u6458\u8981\u751f\u6210\u6216\u60c5\u611f\u5206\u6790\u7b49\u3002\u8fd9\u5c31\u50cf\u4eba\u7c7b\u5728\u638c\u63e1\u57fa\u672c\u8bed\u8a00\u80fd\u529b\u540e\uff0c\u8fdb\u4e00\u6b65\u5b66\u4e60\u4e13\u4e1a\u8bcd\u6c47\u548c\u7279\u5b9a\u9886\u57df\u7684\u8868\u8fbe\u65b9\u5f0f\u3002\u6a21\u578b\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u4f8b\u5b66\u4e60\uff0c\u9010\u6e10\u7406\u89e3\u8bed\u8a00\u4e2d\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u5305\u62ec\u8bed\u5883\u3001\u8bed\u6c14\u3001\u9690\u542b\u610f\u4e49\u7b49\u3002\u8fd9\u4e2a\u5b66\u4e60\u8fc7\u7a0b\u662f\u6301\u7eed\u7684\uff0c\u6a21\u578b\u901a\u8fc7\u4e0d\u65ad\u63a5\u89e6\u65b0\u7684\u8bed\u8a00\u6837\u672c\u6765\u5b8c\u5584\u81ea\u5df1\u7684\u7406\u89e3\u80fd\u529b...\"),\n    \n    HumanMessage(content=\"\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u9047\u5230\u4ec0\u4e48\u6311\u6218\uff1f\"),\n    AIMessage(content=\"\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u7740\u591a\u91cd\u6311\u6218\u3002\u9996\u5148\u662f\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\uff0c\u8bad\u7ec3\u5927\u6a21\u578b\u9700\u8981\u6570\u5343\u53f0GPU\u548c\u6570\u6708\u65f6\u95f4\u3002\u8fd9\u4e0d\u4ec5\u5e26\u6765\u4e86\u5de8\u5927\u7684\u7ecf\u6d4e\u6210\u672c\uff0c\u8fd8\u9762\u4e34\u7740\u80fd\u6e90\u6d88\u8017\u548c\u73af\u5883\u5f71\u54cd\u7684\u95ee\u9898\u3002\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u53ef\u80fd\u6d88\u8017\u6570\u767e\u4e07\u5ea6\u7535\uff0c\u76f8\u5f53\u4e8e\u6570\u5343\u4e2a\u5bb6\u5ead\u4e00\u5e74\u7684\u7528\u7535\u91cf\u3002\u5176\u6b21\u662f\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u83b7\u53d6\u548c\u5904\u7406\u95ee\u9898\u3002\u6a21\u578b\u9700\u8981\u6d77\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5fc5\u987b\u7ecf\u8fc7\u4e25\u683c\u7684\u7b5b\u9009\u548c\u6e05\u6d17\u3002\u6570\u636e\u4e2d\u53ef\u80fd\u5305\u542b\u504f\u89c1\u3001\u6b67\u89c6\u3001\u4e0d\u5f53\u5185\u5bb9\u7b49\u6709\u5bb3\u4fe1\u606f\uff0c\u5982\u679c\u4e0d\u7ecf\u8fc7\u5904\u7406\uff0c\u8fd9\u4e9b\u95ee\u9898\u4f1a\u88ab\u6a21\u578b\u5b66\u4e60\u5e76\u5728\u8f93\u51fa\u4e2d\u4f53\u73b0\u51fa\u6765\u3002\u6b64\u5916\uff0c\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\u4e5f\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u9700\u8981\u786e\u4fdd\u6570\u636e\u80fd\u591f\u8986\u76d6\u4e0d\u540c\u8bed\u8a00\u3001\u6587\u5316\u548c\u9886\u57df\u7684\u77e5\u8bc6\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u8fd8\u9762\u4e34\u7740\u6a21\u578b\u4f18\u5316\u7684\u6280\u672f\u6311\u6218\uff0c\u6bd4\u5982\u68af\u5ea6\u6d88\u5931\u3001\u8fc7\u62df\u5408\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002\u8fd9\u9700\u8981\u7814\u7a76\u4eba\u5458\u4e0d\u65ad\u6539\u8fdb\u8bad\u7ec3\u7b97\u6cd5\u548c\u7b56\u7565\u3002\u53e6\u5916\uff0c\u6a21\u578b\u7684\u77e5\u8bc6\u66f4\u65b0\u4e5f\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u4e16\u754c\u5728\u4e0d\u65ad\u53d8\u5316\uff0c\u65b0\u7684\u4fe1\u606f\u548c\u77e5\u8bc6\u5728\u4e0d\u65ad\u4ea7\u751f\uff0c\u5982\u4f55\u8ba9\u6a21\u578b\u4fdd\u6301\u6700\u65b0\u7684\u77e5\u8bc6\u72b6\u6001\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898...\"),\n    \n    HumanMessage(content=\"\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5982\u4f55\u751f\u6210\u56de\u7b54\u7684\uff1f\"),\n    AIMessage(content=\"\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u56de\u7b54\u7684\u8fc7\u7a0b\u975e\u5e38\u6709\u8da3\u4e14\u590d\u6742\u3002\u5f53\u6a21\u578b\u6536\u5230\u4e00\u4e2a\u95ee\u9898\u6216\u63d0\u793a\u65f6\uff0c\u5b83\u9996\u5148\u4f1a\u901a\u8fc7\u5176\u7f16\u7801\u5668\u5c06\u8f93\u5165\u8f6c\u6362\u4e3a\u9ad8\u7ef4\u5411\u91cf\u8868\u793a\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u4f1a\u8003\u8651\u8f93\u5165\u7684\u6bcf\u4e2a\u8bcd\u8bed\u53ca\u5176\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002\u901a\u8fc7\u591a\u5c42\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6a21\u578b\u80fd\u591f\u7406\u89e3\u8f93\u5165\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u548c\u8bed\u4e49\u7ed3\u6784\u3002\u5728\u751f\u6210\u56de\u7b54\u65f6\uff0c\u6a21\u578b\u4f1a\u4e00\u4e2a\u8bcd\u4e00\u4e2a\u8bcd\u5730\u9884\u6d4b\u6700\u5408\u9002\u7684\u5185\u5bb9\u3002\u6bcf\u751f\u6210\u4e00\u4e2a\u8bcd\uff0c\u90fd\u4f1a\u57fa\u4e8e\u4e4b\u524d\u751f\u6210\u7684\u6240\u6709\u5185\u5bb9\u548c\u539f\u59cb\u8f93\u5165\u6765\u51b3\u5b9a\u4e0b\u4e00\u4e2a\u6700\u5408\u9002\u7684\u8bcd\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4f7f\u7528\u4e86\u590d\u6742\u7684\u6982\u7387\u5206\u5e03\u8ba1\u7b97\uff0c\u6a21\u578b\u4f1a\u4e3a\u8bcd\u8868\u4e2d\u7684\u6bcf\u4e2a\u5019\u9009\u8bcd\u8ba1\u7b97\u4e00\u4e2a\u6982\u7387\u5206\u6570\uff0c\u7136\u540e\u9009\u62e9\u6700\u5408\u9002\u7684\u8bcd\u3002\u8fd9\u79cd\u9009\u62e9\u4e0d\u4ec5\u8981\u8003\u8651\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u8fd8\u8981\u8003\u8651\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002\u6a21\u578b\u8fd8\u4f1a\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u6765\u5173\u6ce8\u8f93\u5165\u4e2d\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u786e\u4fdd\u751f\u6210\u7684\u5185\u5bb9\u4e0e\u95ee\u9898\u76f8\u5173\u3002\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u4f1a\u7ef4\u62a4\u4e00\u4e2a\u5185\u90e8\u72b6\u6001\uff0c\u8fd9\u4e2a\u72b6\u6001\u5305\u542b\u4e86\u4e4b\u524d\u751f\u6210\u5185\u5bb9\u7684\u4fe1\u606f\uff0c\u5e2e\u52a9\u786e\u4fdd\u6574\u4e2a\u56de\u7b54\u7684\u8fde\u8d2f\u6027\u548c\u5b8c\u6574\u6027\u3002\u540c\u65f6\uff0c\u6a21\u578b\u8fd8\u4f1a\u4f7f\u7528\u5404\u79cd\u7b56\u7565\u6765\u63a7\u5236\u751f\u6210\u7684\u8d28\u91cf\uff0c\u6bd4\u5982\u6e29\u5ea6\u53c2\u6570\u6765\u8c03\u8282\u521b\u9020\u6027\uff0c\u6216\u4f7f\u7528\u675f\u641c\u7d22\u7b49\u6280\u672f\u6765\u4f18\u5316\u8f93\u51fa\u8d28\u91cf...\"),\n    \n    HumanMessage(content=\"\u5b83\u4eec\u771f\u7684\u7406\u89e3\u6211\u4eec\u8bf4\u7684\u8bdd\u5417\uff1f\"),\n    AIMessage(content=\"\u8fd9\u6d89\u53ca\u5230AI\u610f\u8bc6\u548c\u7406\u89e3\u529b\u7684\u54f2\u5b66\u95ee\u9898\uff0c\u8fd9\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u5bcc\u6709\u4e89\u8bae\u7684\u8bdd\u9898\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ea7\u751f\u770b\u4f3c\u667a\u80fd\u7684\u56de\u7b54\uff0c\u4f46\u5b83\u4eec\u662f\u5426\u771f\u6b63'\u7406\u89e3'\u4ecd\u7136\u5b58\u5728\u5f88\u5927\u4e89\u8bae\u3002\u4ece\u6280\u672f\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9\u4e9b\u6a21\u578b\u672c\u8d28\u4e0a\u662f\u975e\u5e38\u590d\u6742\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u5b83\u4eec\u901a\u8fc7\u5206\u6790\u5927\u91cf\u6587\u672c\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\u6765\u751f\u6210\u56de\u5e94\u3002\u5b83\u4eec\u7684'\u7406\u89e3'\u66f4\u50cf\u662f\u4e00\u79cd\u9ad8\u7ea7\u7684\u6a21\u5f0f\u5339\u914d\u548c\u6982\u7387\u9884\u6d4b\uff0c\u800c\u4e0d\u662f\u50cf\u4eba\u7c7b\u90a3\u6837\u5177\u6709\u771f\u6b63\u7684\u7406\u89e3\u548c\u610f\u8bc6\u3002\u4eba\u7c7b\u7684\u7406\u89e3\u6d89\u53ca\u5230\u611f\u77e5\u3001\u7ecf\u9a8c\u3001\u60c5\u611f\u548c\u610f\u8bc6\u7b49\u591a\u4e2a\u5c42\u9762\uff0c\u800cAI\u76ee\u524d\u8fd8\u65e0\u6cd5\u771f\u6b63\u590d\u5236\u8fd9\u4e9b\u7279\u8d28\u3002\u4f8b\u5982\uff0c\u5f53\u6211\u4eec\u7406\u89e3'\u82f9\u679c'\u8fd9\u4e2a\u8bcd\u65f6\uff0c\u6211\u4eec\u4e0d\u4ec5\u77e5\u9053\u5b83\u7684\u5b57\u9762\u542b\u4e49\uff0c\u8fd8\u80fd\u8054\u60f3\u5230\u5b83\u7684\u5473\u9053\u3001\u89e6\u611f\u3001\u6c14\u5473\uff0c\u4ee5\u53ca\u4e0e\u4e4b\u76f8\u5173\u7684\u4e2a\u4eba\u7ecf\u5386\u548c\u60c5\u611f\u3002\u800cAI\u6a21\u578b\u53ea\u80fd\u57fa\u4e8e\u6587\u672c\u6570\u636e\u4e2d\u7684\u7edf\u8ba1\u5173\u7cfb\u6765\u5904\u7406\u8fd9\u4e2a\u8bcd\u3002\u6b64\u5916\uff0c\u4eba\u7c7b\u7684\u7406\u89e3\u8fd8\u5305\u62ec\u5e38\u8bc6\u63a8\u7406\u3001\u56e0\u679c\u5173\u7cfb\u7406\u89e3\u3001\u60c5\u5883\u9002\u5e94\u7b49\u80fd\u529b\uff0c\u8fd9\u4e9b\u90fd\u662f\u5f53\u524dAI\u7cfb\u7edf\u8fd8\u96be\u4ee5\u5b8c\u5168\u638c\u63e1\u7684\u3002\u867d\u7136\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u7b26\u5408\u8bed\u6cd5\u548c\u8bed\u4e49\u7684\u56de\u7b54\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u7f3a\u4e4f\u771f\u6b63\u7684\u7406\u89e3\u6df1\u5ea6\uff0c\u6709\u65f6\u4f1a\u4ea7\u751f\u8868\u9762\u4e0a\u5408\u7406\u4f46\u5b9e\u9645\u4e0a\u6ca1\u6709\u610f\u4e49\u7684\u56de\u7b54...\"),\n    \n    HumanMessage(content=\"\u672a\u6765\u5927\u8bed\u8a00\u6a21\u578b\u4f1a\u53d8\u5f97\u66f4\u5f3a\u5927\u5417\uff1f\"),\n    AIMessage(content=\"\u968f\u7740\u6280\u672f\u8fdb\u6b65\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u786e\u5b9e\u5728\u4e0d\u65ad\u63d0\u5347\uff0c\u672a\u6765\u53d1\u5c55\u524d\u666f\u4ee4\u4eba\u671f\u5f85\u3002\u4ece\u6280\u672f\u5c42\u9762\u6765\u770b\uff0c\u6211\u4eec\u6b63\u5728\u89c1\u8bc1\u6a21\u578b\u89c4\u6a21\u7684\u6301\u7eed\u589e\u957f\uff0c\u7b97\u6cd5\u7684\u4e0d\u65ad\u6539\u8fdb\uff0c\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u7684\u6301\u7eed\u6269\u5145\u3002\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u53ef\u80fd\u5305\u62ec\u591a\u4e2a\u7a81\u7834\u6027\u7684\u9886\u57df\uff1a\u9996\u5148\u662f\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u7684\u63d0\u5347\uff0c\u672a\u6765\u7684\u6a21\u578b\u4e0d\u4ec5\u80fd\u5904\u7406\u6587\u672c\uff0c\u8fd8\u80fd\u66f4\u597d\u5730\u7406\u89e3\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u7b49\u591a\u79cd\u5f62\u5f0f\u7684\u4fe1\u606f\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u7406\u89e3\u548c\u751f\u6210\u3002\u5176\u6b21\u662f\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u4e0d\u65ad\u5b66\u4e60\u548c\u66f4\u65b0\u77e5\u8bc6\uff0c\u800c\u4e0d\u662f\u505c\u7559\u5728\u8bad\u7ec3\u65f6\u7684\u77e5\u8bc6\u72b6\u6001\u3002\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\uff0c\u672a\u6765\u7684\u6a21\u578b\u53ef\u80fd\u4f1a\u5177\u5907\u66f4\u5f3a\u7684\u903b\u8f91\u63a8\u7406\u3001\u56e0\u679c\u63a8\u7406\u548c\u62bd\u8c61\u601d\u7ef4\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u95ee\u9898\u548c\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u6a21\u578b\u7684\u6548\u7387\u4e5f\u4f1a\u5f97\u5230\u63d0\u5347\uff0c\u901a\u8fc7\u66f4\u5148\u8fdb\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u80fd\u6e90\u6d88\u8017\u3002\u5728\u5e94\u7528\u9886\u57df\uff0c\u6211\u4eec\u53ef\u80fd\u4f1a\u770b\u5230\u66f4\u4e13\u4e1a\u5316\u7684\u6a21\u578b\u51fa\u73b0\uff0c\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u5982\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u79d1\u7814\u7b49\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u670d\u52a1\u3002\u540c\u65f6\uff0c\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u4e5f\u4f1a\u5f97\u5230\u6539\u5584\uff0c\u4f7f\u5176\u51b3\u7b56\u8fc7\u7a0b\u66f4\u900f\u660e\u3001\u66f4\u53ef\u9760...\"),\n    \n    HumanMessage(content=\"\u6211\u4eec\u5e94\u8be5\u5982\u4f55\u8d1f\u8d23\u4efb\u5730\u4f7f\u7528AI\u6280\u672f\uff1f\"),\n    AIMessage(content=\"\u8fd9\u9700\u8981\u6211\u4eec\u5728\u591a\u4e2a\u5c42\u9762\u505a\u51fa\u52aa\u529b\uff0c\u6784\u5efa\u4e00\u4e2a\u8d1f\u8d23\u4efb\u7684AI\u4f7f\u7528\u751f\u6001\u7cfb\u7edf\u3002\u9996\u5148\u5728\u6280\u672f\u5c42\u9762\uff0c\u6211\u4eec\u9700\u8981\u786e\u4fddAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u63a7\u6027\u3002\u8fd9\u5305\u62ec\u5f00\u53d1\u5f3a\u5927\u7684\u5b89\u5168\u673a\u5236\uff0c\u9632\u6b62\u7cfb\u7edf\u88ab\u6ee5\u7528\u6216\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\uff1b\u5efa\u7acb\u6709\u6548\u7684\u76d1\u6d4b\u548c\u63a7\u5236\u673a\u5236\uff0c\u786e\u4fdd\u7cfb\u7edf\u884c\u4e3a\u7b26\u5408\u9884\u671f\uff1b\u5b9e\u65bd\u4e25\u683c\u7684\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u8bc4\u4f30\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u7a33\u5b9a\u6027\u3002\u5728\u4f26\u7406\u5c42\u9762\uff0c\u6211\u4eec\u9700\u8981\u5efa\u7acb\u5b8c\u5584\u7684\u4f7f\u7528\u51c6\u5219\u548c\u76d1\u7ba1\u6846\u67b6\u3002\u8fd9\u5305\u62ec\u5236\u5b9a\u660e\u786e\u7684AI\u4f26\u7406\u539f\u5219\uff0c\u89c4\u8303AI\u7684\u5f00\u53d1\u548c\u4f7f\u7528\uff1b\u5efa\u7acb\u884c\u4e1a\u6807\u51c6\u548c\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff1b\u8bbe\u7acb\u72ec\u7acb\u7684\u76d1\u7763\u673a\u6784\uff0c\u786e\u4fddAI\u6280\u672f\u7684\u4f7f\u7528\u7b26\u5408\u516c\u5171\u5229\u76ca\u3002\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\uff0c\u6211\u4eec\u9700\u8981\u91c7\u53d6\u4e25\u683c\u7684\u6570\u636e\u4fdd\u62a4\u63aa\u65bd\uff0c\u786e\u4fdd\u7528\u6237\u6570\u636e\u7684\u5b89\u5168\u6027\u548c\u9690\u79c1\u6027\uff1b\u5efa\u7acb\u900f\u660e\u7684\u6570\u636e\u4f7f\u7528\u653f\u7b56\uff1b\u7ed9\u4e88\u7528\u6237\u5bf9\u5176\u6570\u636e\u7684\u63a7\u5236\u6743\u3002\u5728\u504f\u89c1\u6d88\u9664\u65b9\u9762\uff0c\u6211\u4eec\u9700\u8981\u6301\u7eed\u52aa\u529b\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\uff1b\u5f00\u53d1\u66f4\u516c\u5e73\u7684\u7b97\u6cd5\uff1b\u5b9a\u671f\u8bc4\u4f30\u548c\u6d88\u9664\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u3002\u5728\u73af\u5883\u5f71\u54cd\u65b9\u9762\uff0c\u6211\u4eec\u9700\u8981\u5173\u6ce8AI\u7cfb\u7edf\u7684\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\uff1b\u5f00\u53d1\u66f4\u73af\u4fdd\u7684\u8ba1\u7b97\u65b9\u6848\uff1b\u63a8\u52a8\u7eff\u8272AI\u6280\u672f\u7684\u53d1\u5c55...\"),\n    \n    HumanMessage(content=\"\u4f60\u89c9\u5f97AI\u4f1a\u53d6\u4ee3\u4eba\u7c7b\u5417\uff1f\"),\n    AIMessage(content=\"AI\u4e0d\u5e94\u8be5\u4e5f\u4e0d\u4f1a\u5b8c\u5168\u53d6\u4ee3\u4eba\u7c7b\uff0c\u8fd9\u4e2a\u95ee\u9898\u9700\u8981\u4ece\u591a\u4e2a\u89d2\u5ea6\u6df1\u5165\u601d\u8003\u3002\u9996\u5148\uff0c\u867d\u7136AI\u5728\u67d0\u4e9b\u7279\u5b9a\u4efb\u52a1\u4e0a\u53ef\u80fd\u8d85\u8d8a\u4eba\u7c7b\uff0c\u4f46\u4eba\u7c7b\u5177\u6709\u8bb8\u591aAI\u96be\u4ee5\u590d\u5236\u7684\u72ec\u7279\u4f18\u52bf\u3002\u4eba\u7c7b\u7684\u521b\u9020\u529b\u662f\u72ec\u7279\u7684\uff0c\u6211\u4eec\u80fd\u591f\u4ea7\u751f\u539f\u521b\u6027\u7684\u60f3\u6cd5\uff0c\u8fdb\u884c\u827a\u672f\u521b\u4f5c\uff0c\u63d0\u51fa\u521b\u65b0\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4eba\u7c7b\u7684\u60c5\u611f\u5171\u9e23\u80fd\u529b\u4e5f\u662f\u65e0\u53ef\u66ff\u4ee3\u7684\uff0c\u6211\u4eec\u80fd\u591f\u7406\u89e3\u548c\u5206\u4eab\u4ed6\u4eba\u7684\u60c5\u611f\uff0c\u5efa\u7acb\u6df1\u5c42\u7684\u60c5\u611f\u8054\u7cfb\uff0c\u8fd9\u662f\u5f53\u524dAI\u6280\u672f\u8fdc\u8fdc\u65e0\u6cd5\u8fbe\u5230\u7684\u3002\u5728\u9053\u5fb7\u5224\u65ad\u548c\u4ef7\u503c\u89c2\u65b9\u9762\uff0c\u4eba\u7c7b\u80fd\u591f\u57fa\u4e8e\u590d\u6742\u7684\u4f26\u7406\u8003\u8651\u505a\u51fa\u51b3\u7b56\uff0c\u800cAI\u7cfb\u7edf\u5f80\u5f80\u96be\u4ee5\u5904\u7406\u9700\u8981\u9053\u5fb7\u6743\u8861\u7684\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u4eba\u7c7b\u5177\u6709\u81ea\u4e3b\u610f\u8bc6\u548c\u4e3b\u89c2\u4f53\u9a8c\uff0c\u8fd9\u4e9b\u90fd\u662fAI\u6240\u4e0d\u5177\u5907\u7684\u7279\u8d28\u3002\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u5e94\u8be5\u662f\u4eba\u673a\u534f\u4f5c\uff0c\u8ba9AI\u6210\u4e3a\u589e\u5f3a\u4eba\u7c7b\u80fd\u529b\u7684\u5de5\u5177\uff0c\u800c\u4e0d\u662f\u66ff\u4ee3\u54c1\u3002\u5728\u8fd9\u79cd\u534f\u4f5c\u6a21\u5f0f\u4e0b\uff0cAI\u53ef\u4ee5\u5904\u7406\u91cd\u590d\u6027\u3001\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u4efb\u52a1\uff0c\u800c\u4eba\u7c7b\u5219\u4e13\u6ce8\u4e8e\u9700\u8981\u521b\u9020\u529b\u3001\u60c5\u611f\u7406\u89e3\u548c\u9053\u5fb7\u5224\u65ad\u7684\u5de5\u4f5c\u3002\u6211\u4eec\u9700\u8981\u660e\u667a\u5730\u4f7f\u7528AI\u6280\u672f\uff0c\u786e\u4fdd\u5b83\u59cb\u7ec8\u670d\u52a1\u4e8e\u4eba\u7c7b\u798f\u7949\uff0c\u800c\u4e0d\u662f\u53cd\u8fc7\u6765\u63a7\u5236\u6216\u9650\u5236\u4eba\u7c7b\u7684\u53d1\u5c55...\"),\n    HumanMessage(content=\"\u4f60\u8ba4\u4e3a\u672a\u6765\u7684AI\u4f1a\u600e\u4e48\u53d1\u5c55\uff1f\")\n]\n</pre> messages = [     SystemMessage(content=\"\u4f60\u662f\u4e00\u4e2a\u53cb\u597d\u7684AI\u52a9\u624b\"),     HumanMessage(content=\"\u4f60\u80fd\u7ed9\u6211\u8bb2\u4e00\u4e2a\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6545\u4e8b\u5417\uff1f\"),     AIMessage(content=\"\u5f53\u7136\u53ef\u4ee5\u3002\u8ba9\u6211\u4eec\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8d77\u6e90\u5f00\u59cb\u8bb2\u8d77\u3002\u4e00\u5207\u8981\u4ece2017\u5e74\u8c37\u6b4c\u63d0\u51fa\u7684Transformer\u67b6\u6784\u8bf4\u8d77\u3002\u8fd9\u4e2a\u521b\u65b0\u6027\u7684\u67b6\u6784\u4e3a\u540e\u6765\u7684GPT\u3001BERT\u7b49\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002Transformer\u67b6\u6784\u5f15\u5165\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3002\u8fd9\u4e00\u7a81\u7834\u6027\u8fdb\u5c55\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u6587\u672c\u7684\u4e0a\u4e0b\u6587\u8bed\u5883\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u5e26\u6765\u4e86\u9769\u547d\u6027\u7684\u53d8\u5316\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0cOpenAI\u4e8e2018\u5e74\u53d1\u5e03\u4e86\u7b2c\u4e00\u4ee3GPT\u6a21\u578b\uff0c\u968f\u540e\u53c8\u76f8\u7ee7\u63a8\u51fa\u4e86GPT-2\u548cGPT-3\uff0c\u6bcf\u4e00\u4ee3\u90fd\u5728\u89c4\u6a21\u548c\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002\u540c\u65f6\uff0c\u8c37\u6b4c\u63a8\u51fa\u7684BERT\u6a21\u578b\u91c7\u7528\u4e86\u53cc\u5411\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5728\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002\u8fd9\u4e9b\u6a21\u578b\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u66f4\u591a\u7814\u7a76\u8005\u548c\u673a\u6784\u6295\u5165\u5230\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7814\u53d1\u4e2d\uff0c\u63a8\u52a8\u4e86\u6574\u4e2a\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\u3002\u73b0\u5728\uff0c\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u50cfGPT-4\u8fd9\u6837\u7684\u6a21\u578b\u5c55\u73b0\u51fa\u4ee4\u4eba\u60ca\u53f9\u7684\u80fd\u529b\uff0c\u4e0d\u4ec5\u80fd\u591f\u8fdb\u884c\u57fa\u7840\u7684\u6587\u672c\u751f\u6210\uff0c\u8fd8\u80fd\u591f\u7406\u89e3\u4e0a\u4e0b\u6587\u3001\u8fdb\u884c\u63a8\u7406\u3001\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u751a\u81f3\u5c55\u73b0\u51fa\u4e00\u5b9a\u7a0b\u5ea6\u7684\u521b\u9020\u529b...\"),          HumanMessage(content=\"\u90a3AI\u662f\u5982\u4f55\u5b66\u4e60\u7406\u89e3\u4eba\u7c7b\u8bed\u8a00\u7684\u5462\uff1f\"),     AIMessage(content=\"\u8fd9\u662f\u4e2a\u5f88\u597d\u7684\u95ee\u9898\u3002AI\u901a\u8fc7\u5927\u91cf\u7684\u6587\u672c\u6570\u636e\u8bad\u7ec3\u6765\u7406\u89e3\u8bed\u8a00\u3002\u5b83\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u6349\u8bcd\u8bed\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u4e2a\u9636\u6bb5\uff0c\u9010\u6b65\u638c\u63e1\u8bed\u8a00\u7684\u89c4\u5f8b\u3002\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u6a21\u578b\u4f1a\u9605\u8bfb\u6d77\u91cf\u7684\u6587\u672c\uff0c\u5b66\u4e60\u8bed\u8a00\u7684\u57fa\u672c\u6a21\u5f0f\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u50cf\u4e00\u4e2a\u5a74\u513f\u901a\u8fc7\u89c2\u5bdf\u548c\u6a21\u4eff\u6765\u5b66\u4e60\u8bed\u8a00\u4e00\u6837\u3002\u6a21\u578b\u4f1a\u5206\u6790\u6570\u5341\u4ebf\u751a\u81f3\u6570\u5343\u4ebf\u4e2a\u8bcd\u8bed\uff0c\u7406\u89e3\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u8054\u548c\u4f7f\u7528\u89c4\u5f8b\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u4f1a\u5efa\u7acb\u8d77\u4e00\u4e2a\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u6bcf\u4e2a\u795e\u7ecf\u5143\u90fd\u8d1f\u8d23\u6355\u6349\u7279\u5b9a\u7684\u8bed\u8a00\u7279\u5f81\u3002\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff0c\u6a21\u578b\u4e0d\u65ad\u8c03\u6574\u5176\u5185\u90e8\u53c2\u6570\uff0c\u4ee5\u66f4\u597d\u5730\u9884\u6d4b\u548c\u7406\u89e3\u8bed\u8a00\u3002\u5728\u5fae\u8c03\u9636\u6bb5\uff0c\u6a21\u578b\u4f1a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3\uff0c\u6bd4\u5982\u95ee\u7b54\u3001\u6458\u8981\u751f\u6210\u6216\u60c5\u611f\u5206\u6790\u7b49\u3002\u8fd9\u5c31\u50cf\u4eba\u7c7b\u5728\u638c\u63e1\u57fa\u672c\u8bed\u8a00\u80fd\u529b\u540e\uff0c\u8fdb\u4e00\u6b65\u5b66\u4e60\u4e13\u4e1a\u8bcd\u6c47\u548c\u7279\u5b9a\u9886\u57df\u7684\u8868\u8fbe\u65b9\u5f0f\u3002\u6a21\u578b\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u4f8b\u5b66\u4e60\uff0c\u9010\u6e10\u7406\u89e3\u8bed\u8a00\u4e2d\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u5305\u62ec\u8bed\u5883\u3001\u8bed\u6c14\u3001\u9690\u542b\u610f\u4e49\u7b49\u3002\u8fd9\u4e2a\u5b66\u4e60\u8fc7\u7a0b\u662f\u6301\u7eed\u7684\uff0c\u6a21\u578b\u901a\u8fc7\u4e0d\u65ad\u63a5\u89e6\u65b0\u7684\u8bed\u8a00\u6837\u672c\u6765\u5b8c\u5584\u81ea\u5df1\u7684\u7406\u89e3\u80fd\u529b...\"),          HumanMessage(content=\"\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u9047\u5230\u4ec0\u4e48\u6311\u6218\uff1f\"),     AIMessage(content=\"\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u7740\u591a\u91cd\u6311\u6218\u3002\u9996\u5148\u662f\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\uff0c\u8bad\u7ec3\u5927\u6a21\u578b\u9700\u8981\u6570\u5343\u53f0GPU\u548c\u6570\u6708\u65f6\u95f4\u3002\u8fd9\u4e0d\u4ec5\u5e26\u6765\u4e86\u5de8\u5927\u7684\u7ecf\u6d4e\u6210\u672c\uff0c\u8fd8\u9762\u4e34\u7740\u80fd\u6e90\u6d88\u8017\u548c\u73af\u5883\u5f71\u54cd\u7684\u95ee\u9898\u3002\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u53ef\u80fd\u6d88\u8017\u6570\u767e\u4e07\u5ea6\u7535\uff0c\u76f8\u5f53\u4e8e\u6570\u5343\u4e2a\u5bb6\u5ead\u4e00\u5e74\u7684\u7528\u7535\u91cf\u3002\u5176\u6b21\u662f\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u83b7\u53d6\u548c\u5904\u7406\u95ee\u9898\u3002\u6a21\u578b\u9700\u8981\u6d77\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5fc5\u987b\u7ecf\u8fc7\u4e25\u683c\u7684\u7b5b\u9009\u548c\u6e05\u6d17\u3002\u6570\u636e\u4e2d\u53ef\u80fd\u5305\u542b\u504f\u89c1\u3001\u6b67\u89c6\u3001\u4e0d\u5f53\u5185\u5bb9\u7b49\u6709\u5bb3\u4fe1\u606f\uff0c\u5982\u679c\u4e0d\u7ecf\u8fc7\u5904\u7406\uff0c\u8fd9\u4e9b\u95ee\u9898\u4f1a\u88ab\u6a21\u578b\u5b66\u4e60\u5e76\u5728\u8f93\u51fa\u4e2d\u4f53\u73b0\u51fa\u6765\u3002\u6b64\u5916\uff0c\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\u4e5f\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u9700\u8981\u786e\u4fdd\u6570\u636e\u80fd\u591f\u8986\u76d6\u4e0d\u540c\u8bed\u8a00\u3001\u6587\u5316\u548c\u9886\u57df\u7684\u77e5\u8bc6\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u8fd8\u9762\u4e34\u7740\u6a21\u578b\u4f18\u5316\u7684\u6280\u672f\u6311\u6218\uff0c\u6bd4\u5982\u68af\u5ea6\u6d88\u5931\u3001\u8fc7\u62df\u5408\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002\u8fd9\u9700\u8981\u7814\u7a76\u4eba\u5458\u4e0d\u65ad\u6539\u8fdb\u8bad\u7ec3\u7b97\u6cd5\u548c\u7b56\u7565\u3002\u53e6\u5916\uff0c\u6a21\u578b\u7684\u77e5\u8bc6\u66f4\u65b0\u4e5f\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u4e16\u754c\u5728\u4e0d\u65ad\u53d8\u5316\uff0c\u65b0\u7684\u4fe1\u606f\u548c\u77e5\u8bc6\u5728\u4e0d\u65ad\u4ea7\u751f\uff0c\u5982\u4f55\u8ba9\u6a21\u578b\u4fdd\u6301\u6700\u65b0\u7684\u77e5\u8bc6\u72b6\u6001\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898...\"),          HumanMessage(content=\"\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5982\u4f55\u751f\u6210\u56de\u7b54\u7684\uff1f\"),     AIMessage(content=\"\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u56de\u7b54\u7684\u8fc7\u7a0b\u975e\u5e38\u6709\u8da3\u4e14\u590d\u6742\u3002\u5f53\u6a21\u578b\u6536\u5230\u4e00\u4e2a\u95ee\u9898\u6216\u63d0\u793a\u65f6\uff0c\u5b83\u9996\u5148\u4f1a\u901a\u8fc7\u5176\u7f16\u7801\u5668\u5c06\u8f93\u5165\u8f6c\u6362\u4e3a\u9ad8\u7ef4\u5411\u91cf\u8868\u793a\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u4f1a\u8003\u8651\u8f93\u5165\u7684\u6bcf\u4e2a\u8bcd\u8bed\u53ca\u5176\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002\u901a\u8fc7\u591a\u5c42\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6a21\u578b\u80fd\u591f\u7406\u89e3\u8f93\u5165\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u548c\u8bed\u4e49\u7ed3\u6784\u3002\u5728\u751f\u6210\u56de\u7b54\u65f6\uff0c\u6a21\u578b\u4f1a\u4e00\u4e2a\u8bcd\u4e00\u4e2a\u8bcd\u5730\u9884\u6d4b\u6700\u5408\u9002\u7684\u5185\u5bb9\u3002\u6bcf\u751f\u6210\u4e00\u4e2a\u8bcd\uff0c\u90fd\u4f1a\u57fa\u4e8e\u4e4b\u524d\u751f\u6210\u7684\u6240\u6709\u5185\u5bb9\u548c\u539f\u59cb\u8f93\u5165\u6765\u51b3\u5b9a\u4e0b\u4e00\u4e2a\u6700\u5408\u9002\u7684\u8bcd\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4f7f\u7528\u4e86\u590d\u6742\u7684\u6982\u7387\u5206\u5e03\u8ba1\u7b97\uff0c\u6a21\u578b\u4f1a\u4e3a\u8bcd\u8868\u4e2d\u7684\u6bcf\u4e2a\u5019\u9009\u8bcd\u8ba1\u7b97\u4e00\u4e2a\u6982\u7387\u5206\u6570\uff0c\u7136\u540e\u9009\u62e9\u6700\u5408\u9002\u7684\u8bcd\u3002\u8fd9\u79cd\u9009\u62e9\u4e0d\u4ec5\u8981\u8003\u8651\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u8fd8\u8981\u8003\u8651\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002\u6a21\u578b\u8fd8\u4f1a\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u6765\u5173\u6ce8\u8f93\u5165\u4e2d\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u786e\u4fdd\u751f\u6210\u7684\u5185\u5bb9\u4e0e\u95ee\u9898\u76f8\u5173\u3002\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u4f1a\u7ef4\u62a4\u4e00\u4e2a\u5185\u90e8\u72b6\u6001\uff0c\u8fd9\u4e2a\u72b6\u6001\u5305\u542b\u4e86\u4e4b\u524d\u751f\u6210\u5185\u5bb9\u7684\u4fe1\u606f\uff0c\u5e2e\u52a9\u786e\u4fdd\u6574\u4e2a\u56de\u7b54\u7684\u8fde\u8d2f\u6027\u548c\u5b8c\u6574\u6027\u3002\u540c\u65f6\uff0c\u6a21\u578b\u8fd8\u4f1a\u4f7f\u7528\u5404\u79cd\u7b56\u7565\u6765\u63a7\u5236\u751f\u6210\u7684\u8d28\u91cf\uff0c\u6bd4\u5982\u6e29\u5ea6\u53c2\u6570\u6765\u8c03\u8282\u521b\u9020\u6027\uff0c\u6216\u4f7f\u7528\u675f\u641c\u7d22\u7b49\u6280\u672f\u6765\u4f18\u5316\u8f93\u51fa\u8d28\u91cf...\"),          HumanMessage(content=\"\u5b83\u4eec\u771f\u7684\u7406\u89e3\u6211\u4eec\u8bf4\u7684\u8bdd\u5417\uff1f\"),     AIMessage(content=\"\u8fd9\u6d89\u53ca\u5230AI\u610f\u8bc6\u548c\u7406\u89e3\u529b\u7684\u54f2\u5b66\u95ee\u9898\uff0c\u8fd9\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u5bcc\u6709\u4e89\u8bae\u7684\u8bdd\u9898\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ea7\u751f\u770b\u4f3c\u667a\u80fd\u7684\u56de\u7b54\uff0c\u4f46\u5b83\u4eec\u662f\u5426\u771f\u6b63'\u7406\u89e3'\u4ecd\u7136\u5b58\u5728\u5f88\u5927\u4e89\u8bae\u3002\u4ece\u6280\u672f\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9\u4e9b\u6a21\u578b\u672c\u8d28\u4e0a\u662f\u975e\u5e38\u590d\u6742\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u5b83\u4eec\u901a\u8fc7\u5206\u6790\u5927\u91cf\u6587\u672c\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\u6765\u751f\u6210\u56de\u5e94\u3002\u5b83\u4eec\u7684'\u7406\u89e3'\u66f4\u50cf\u662f\u4e00\u79cd\u9ad8\u7ea7\u7684\u6a21\u5f0f\u5339\u914d\u548c\u6982\u7387\u9884\u6d4b\uff0c\u800c\u4e0d\u662f\u50cf\u4eba\u7c7b\u90a3\u6837\u5177\u6709\u771f\u6b63\u7684\u7406\u89e3\u548c\u610f\u8bc6\u3002\u4eba\u7c7b\u7684\u7406\u89e3\u6d89\u53ca\u5230\u611f\u77e5\u3001\u7ecf\u9a8c\u3001\u60c5\u611f\u548c\u610f\u8bc6\u7b49\u591a\u4e2a\u5c42\u9762\uff0c\u800cAI\u76ee\u524d\u8fd8\u65e0\u6cd5\u771f\u6b63\u590d\u5236\u8fd9\u4e9b\u7279\u8d28\u3002\u4f8b\u5982\uff0c\u5f53\u6211\u4eec\u7406\u89e3'\u82f9\u679c'\u8fd9\u4e2a\u8bcd\u65f6\uff0c\u6211\u4eec\u4e0d\u4ec5\u77e5\u9053\u5b83\u7684\u5b57\u9762\u542b\u4e49\uff0c\u8fd8\u80fd\u8054\u60f3\u5230\u5b83\u7684\u5473\u9053\u3001\u89e6\u611f\u3001\u6c14\u5473\uff0c\u4ee5\u53ca\u4e0e\u4e4b\u76f8\u5173\u7684\u4e2a\u4eba\u7ecf\u5386\u548c\u60c5\u611f\u3002\u800cAI\u6a21\u578b\u53ea\u80fd\u57fa\u4e8e\u6587\u672c\u6570\u636e\u4e2d\u7684\u7edf\u8ba1\u5173\u7cfb\u6765\u5904\u7406\u8fd9\u4e2a\u8bcd\u3002\u6b64\u5916\uff0c\u4eba\u7c7b\u7684\u7406\u89e3\u8fd8\u5305\u62ec\u5e38\u8bc6\u63a8\u7406\u3001\u56e0\u679c\u5173\u7cfb\u7406\u89e3\u3001\u60c5\u5883\u9002\u5e94\u7b49\u80fd\u529b\uff0c\u8fd9\u4e9b\u90fd\u662f\u5f53\u524dAI\u7cfb\u7edf\u8fd8\u96be\u4ee5\u5b8c\u5168\u638c\u63e1\u7684\u3002\u867d\u7136\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u7b26\u5408\u8bed\u6cd5\u548c\u8bed\u4e49\u7684\u56de\u7b54\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u7f3a\u4e4f\u771f\u6b63\u7684\u7406\u89e3\u6df1\u5ea6\uff0c\u6709\u65f6\u4f1a\u4ea7\u751f\u8868\u9762\u4e0a\u5408\u7406\u4f46\u5b9e\u9645\u4e0a\u6ca1\u6709\u610f\u4e49\u7684\u56de\u7b54...\"),          HumanMessage(content=\"\u672a\u6765\u5927\u8bed\u8a00\u6a21\u578b\u4f1a\u53d8\u5f97\u66f4\u5f3a\u5927\u5417\uff1f\"),     AIMessage(content=\"\u968f\u7740\u6280\u672f\u8fdb\u6b65\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u786e\u5b9e\u5728\u4e0d\u65ad\u63d0\u5347\uff0c\u672a\u6765\u53d1\u5c55\u524d\u666f\u4ee4\u4eba\u671f\u5f85\u3002\u4ece\u6280\u672f\u5c42\u9762\u6765\u770b\uff0c\u6211\u4eec\u6b63\u5728\u89c1\u8bc1\u6a21\u578b\u89c4\u6a21\u7684\u6301\u7eed\u589e\u957f\uff0c\u7b97\u6cd5\u7684\u4e0d\u65ad\u6539\u8fdb\uff0c\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u7684\u6301\u7eed\u6269\u5145\u3002\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u53ef\u80fd\u5305\u62ec\u591a\u4e2a\u7a81\u7834\u6027\u7684\u9886\u57df\uff1a\u9996\u5148\u662f\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u7684\u63d0\u5347\uff0c\u672a\u6765\u7684\u6a21\u578b\u4e0d\u4ec5\u80fd\u5904\u7406\u6587\u672c\uff0c\u8fd8\u80fd\u66f4\u597d\u5730\u7406\u89e3\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u7b49\u591a\u79cd\u5f62\u5f0f\u7684\u4fe1\u606f\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u7406\u89e3\u548c\u751f\u6210\u3002\u5176\u6b21\u662f\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u4e0d\u65ad\u5b66\u4e60\u548c\u66f4\u65b0\u77e5\u8bc6\uff0c\u800c\u4e0d\u662f\u505c\u7559\u5728\u8bad\u7ec3\u65f6\u7684\u77e5\u8bc6\u72b6\u6001\u3002\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\uff0c\u672a\u6765\u7684\u6a21\u578b\u53ef\u80fd\u4f1a\u5177\u5907\u66f4\u5f3a\u7684\u903b\u8f91\u63a8\u7406\u3001\u56e0\u679c\u63a8\u7406\u548c\u62bd\u8c61\u601d\u7ef4\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u95ee\u9898\u548c\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u6a21\u578b\u7684\u6548\u7387\u4e5f\u4f1a\u5f97\u5230\u63d0\u5347\uff0c\u901a\u8fc7\u66f4\u5148\u8fdb\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u80fd\u6e90\u6d88\u8017\u3002\u5728\u5e94\u7528\u9886\u57df\uff0c\u6211\u4eec\u53ef\u80fd\u4f1a\u770b\u5230\u66f4\u4e13\u4e1a\u5316\u7684\u6a21\u578b\u51fa\u73b0\uff0c\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u5982\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u79d1\u7814\u7b49\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u670d\u52a1\u3002\u540c\u65f6\uff0c\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u4e5f\u4f1a\u5f97\u5230\u6539\u5584\uff0c\u4f7f\u5176\u51b3\u7b56\u8fc7\u7a0b\u66f4\u900f\u660e\u3001\u66f4\u53ef\u9760...\"),          HumanMessage(content=\"\u6211\u4eec\u5e94\u8be5\u5982\u4f55\u8d1f\u8d23\u4efb\u5730\u4f7f\u7528AI\u6280\u672f\uff1f\"),     AIMessage(content=\"\u8fd9\u9700\u8981\u6211\u4eec\u5728\u591a\u4e2a\u5c42\u9762\u505a\u51fa\u52aa\u529b\uff0c\u6784\u5efa\u4e00\u4e2a\u8d1f\u8d23\u4efb\u7684AI\u4f7f\u7528\u751f\u6001\u7cfb\u7edf\u3002\u9996\u5148\u5728\u6280\u672f\u5c42\u9762\uff0c\u6211\u4eec\u9700\u8981\u786e\u4fddAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u63a7\u6027\u3002\u8fd9\u5305\u62ec\u5f00\u53d1\u5f3a\u5927\u7684\u5b89\u5168\u673a\u5236\uff0c\u9632\u6b62\u7cfb\u7edf\u88ab\u6ee5\u7528\u6216\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\uff1b\u5efa\u7acb\u6709\u6548\u7684\u76d1\u6d4b\u548c\u63a7\u5236\u673a\u5236\uff0c\u786e\u4fdd\u7cfb\u7edf\u884c\u4e3a\u7b26\u5408\u9884\u671f\uff1b\u5b9e\u65bd\u4e25\u683c\u7684\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u8bc4\u4f30\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u7a33\u5b9a\u6027\u3002\u5728\u4f26\u7406\u5c42\u9762\uff0c\u6211\u4eec\u9700\u8981\u5efa\u7acb\u5b8c\u5584\u7684\u4f7f\u7528\u51c6\u5219\u548c\u76d1\u7ba1\u6846\u67b6\u3002\u8fd9\u5305\u62ec\u5236\u5b9a\u660e\u786e\u7684AI\u4f26\u7406\u539f\u5219\uff0c\u89c4\u8303AI\u7684\u5f00\u53d1\u548c\u4f7f\u7528\uff1b\u5efa\u7acb\u884c\u4e1a\u6807\u51c6\u548c\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff1b\u8bbe\u7acb\u72ec\u7acb\u7684\u76d1\u7763\u673a\u6784\uff0c\u786e\u4fddAI\u6280\u672f\u7684\u4f7f\u7528\u7b26\u5408\u516c\u5171\u5229\u76ca\u3002\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\uff0c\u6211\u4eec\u9700\u8981\u91c7\u53d6\u4e25\u683c\u7684\u6570\u636e\u4fdd\u62a4\u63aa\u65bd\uff0c\u786e\u4fdd\u7528\u6237\u6570\u636e\u7684\u5b89\u5168\u6027\u548c\u9690\u79c1\u6027\uff1b\u5efa\u7acb\u900f\u660e\u7684\u6570\u636e\u4f7f\u7528\u653f\u7b56\uff1b\u7ed9\u4e88\u7528\u6237\u5bf9\u5176\u6570\u636e\u7684\u63a7\u5236\u6743\u3002\u5728\u504f\u89c1\u6d88\u9664\u65b9\u9762\uff0c\u6211\u4eec\u9700\u8981\u6301\u7eed\u52aa\u529b\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\uff1b\u5f00\u53d1\u66f4\u516c\u5e73\u7684\u7b97\u6cd5\uff1b\u5b9a\u671f\u8bc4\u4f30\u548c\u6d88\u9664\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u3002\u5728\u73af\u5883\u5f71\u54cd\u65b9\u9762\uff0c\u6211\u4eec\u9700\u8981\u5173\u6ce8AI\u7cfb\u7edf\u7684\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\uff1b\u5f00\u53d1\u66f4\u73af\u4fdd\u7684\u8ba1\u7b97\u65b9\u6848\uff1b\u63a8\u52a8\u7eff\u8272AI\u6280\u672f\u7684\u53d1\u5c55...\"),          HumanMessage(content=\"\u4f60\u89c9\u5f97AI\u4f1a\u53d6\u4ee3\u4eba\u7c7b\u5417\uff1f\"),     AIMessage(content=\"AI\u4e0d\u5e94\u8be5\u4e5f\u4e0d\u4f1a\u5b8c\u5168\u53d6\u4ee3\u4eba\u7c7b\uff0c\u8fd9\u4e2a\u95ee\u9898\u9700\u8981\u4ece\u591a\u4e2a\u89d2\u5ea6\u6df1\u5165\u601d\u8003\u3002\u9996\u5148\uff0c\u867d\u7136AI\u5728\u67d0\u4e9b\u7279\u5b9a\u4efb\u52a1\u4e0a\u53ef\u80fd\u8d85\u8d8a\u4eba\u7c7b\uff0c\u4f46\u4eba\u7c7b\u5177\u6709\u8bb8\u591aAI\u96be\u4ee5\u590d\u5236\u7684\u72ec\u7279\u4f18\u52bf\u3002\u4eba\u7c7b\u7684\u521b\u9020\u529b\u662f\u72ec\u7279\u7684\uff0c\u6211\u4eec\u80fd\u591f\u4ea7\u751f\u539f\u521b\u6027\u7684\u60f3\u6cd5\uff0c\u8fdb\u884c\u827a\u672f\u521b\u4f5c\uff0c\u63d0\u51fa\u521b\u65b0\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4eba\u7c7b\u7684\u60c5\u611f\u5171\u9e23\u80fd\u529b\u4e5f\u662f\u65e0\u53ef\u66ff\u4ee3\u7684\uff0c\u6211\u4eec\u80fd\u591f\u7406\u89e3\u548c\u5206\u4eab\u4ed6\u4eba\u7684\u60c5\u611f\uff0c\u5efa\u7acb\u6df1\u5c42\u7684\u60c5\u611f\u8054\u7cfb\uff0c\u8fd9\u662f\u5f53\u524dAI\u6280\u672f\u8fdc\u8fdc\u65e0\u6cd5\u8fbe\u5230\u7684\u3002\u5728\u9053\u5fb7\u5224\u65ad\u548c\u4ef7\u503c\u89c2\u65b9\u9762\uff0c\u4eba\u7c7b\u80fd\u591f\u57fa\u4e8e\u590d\u6742\u7684\u4f26\u7406\u8003\u8651\u505a\u51fa\u51b3\u7b56\uff0c\u800cAI\u7cfb\u7edf\u5f80\u5f80\u96be\u4ee5\u5904\u7406\u9700\u8981\u9053\u5fb7\u6743\u8861\u7684\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u4eba\u7c7b\u5177\u6709\u81ea\u4e3b\u610f\u8bc6\u548c\u4e3b\u89c2\u4f53\u9a8c\uff0c\u8fd9\u4e9b\u90fd\u662fAI\u6240\u4e0d\u5177\u5907\u7684\u7279\u8d28\u3002\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u5e94\u8be5\u662f\u4eba\u673a\u534f\u4f5c\uff0c\u8ba9AI\u6210\u4e3a\u589e\u5f3a\u4eba\u7c7b\u80fd\u529b\u7684\u5de5\u5177\uff0c\u800c\u4e0d\u662f\u66ff\u4ee3\u54c1\u3002\u5728\u8fd9\u79cd\u534f\u4f5c\u6a21\u5f0f\u4e0b\uff0cAI\u53ef\u4ee5\u5904\u7406\u91cd\u590d\u6027\u3001\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u4efb\u52a1\uff0c\u800c\u4eba\u7c7b\u5219\u4e13\u6ce8\u4e8e\u9700\u8981\u521b\u9020\u529b\u3001\u60c5\u611f\u7406\u89e3\u548c\u9053\u5fb7\u5224\u65ad\u7684\u5de5\u4f5c\u3002\u6211\u4eec\u9700\u8981\u660e\u667a\u5730\u4f7f\u7528AI\u6280\u672f\uff0c\u786e\u4fdd\u5b83\u59cb\u7ec8\u670d\u52a1\u4e8e\u4eba\u7c7b\u798f\u7949\uff0c\u800c\u4e0d\u662f\u53cd\u8fc7\u6765\u63a7\u5236\u6216\u9650\u5236\u4eba\u7c7b\u7684\u53d1\u5c55...\"),     HumanMessage(content=\"\u4f60\u8ba4\u4e3a\u672a\u6765\u7684AI\u4f1a\u600e\u4e48\u53d1\u5c55\uff1f\") ] In\u00a0[5]: Copied! <pre>_input = {\n    \"messages\": messages,\n    \"parent_id\": \"some-parent-id\",\n    \"date\": date.today(),  # noqa: DTZ011\n}\n\ntry:\n    await agent_without_truncation.ainvoke(input=_input)\nexcept Exception as e:\n    print(e)\n</pre> _input = {     \"messages\": messages,     \"parent_id\": \"some-parent-id\",     \"date\": date.today(),  # noqa: DTZ011 }  try:     await agent_without_truncation.ainvoke(input=_input) except Exception as e:     print(e) <pre>Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 1024 tokens. However, you requested 2406 tokens (2150 in the messages, 256 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n</pre> <p>In the following, we use messages length as the truncation method</p> <p>For custom trim settings based on your LLM service(e.g. vLLM,TGI,SGLang), see this example or implement it in your own custom manner.</p> In\u00a0[6]: Copied! <pre># import truncation config\nfrom tablegpt.agent.data_analyzer import TruncationConfig\n</pre> # import truncation config from tablegpt.agent.data_analyzer import TruncationConfig In\u00a0[7]: Copied! <pre># token_counter=len, uses message length as truncation method\n# max_tokens=5, maximum length of messages after truncation\n# start_on=\"human\", start truncation from human messages\nllm_truncation_config = TruncationConfig(token_counter=len, max_tokens=5, start_on=\"human\")\n</pre> # token_counter=len, uses message length as truncation method # max_tokens=5, maximum length of messages after truncation # start_on=\"human\", start truncation from human messages llm_truncation_config = TruncationConfig(token_counter=len, max_tokens=5, start_on=\"human\") In\u00a0[8]: Copied! <pre>agent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    llm_truncation_config=llm_truncation_config\n)\n</pre> agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     llm_truncation_config=llm_truncation_config ) In\u00a0[9]: Copied! <pre>_input = {\n    \"messages\": messages,\n    \"parent_id\": \"some-parent-id\",\n    \"date\": date.today(),  # noqa: DTZ011\n}\ntry:\n    res = await agent.ainvoke(input=_input)\n    print(res[\"messages\"][-1].content)\nexcept Exception as e:\n    print(e)\n</pre> _input = {     \"messages\": messages,     \"parent_id\": \"some-parent-id\",     \"date\": date.today(),  # noqa: DTZ011 } try:     res = await agent.ainvoke(input=_input)     print(res[\"messages\"][-1].content) except Exception as e:     print(e) <pre>\u672a\u6765AI\u7684\u53d1\u5c55\u53ef\u80fd\u4f1a\u57fa\u4e8e\u4e00\u7cfb\u5217\u5148\u8fdb\u7684\u6280\u672f\u548c\u79d1\u5b66\u7a81\u7834\uff0c\u4ee5\u4e0b\u662f\u4e00\u4e9b\u53ef\u80fd\u7684\u53d1\u5c55\u65b9\u5411\uff1a\n\n1. **\u589e\u5f3a\u73b0\u5b9e\u4e0e\u865a\u62df\u73b0\u5b9e**\uff1aAI\u5c06\u80fd\u591f\u63d0\u4f9b\u66f4\u52a0\u6c89\u6d78\u5f0f\u7684\u4f53\u9a8c\uff0c\u4f8b\u5982\u589e\u5f3a\u73b0\u5b9e\u548c\u865a\u62df\u73b0\u5b9e\u6280\u672f\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u66f4\u81ea\u7136\u5730\u4e0e\u865a\u62df\u73af\u5883\u4e92\u52a8\u3002\u8fd9\u5c06\u6539\u53d8\u6211\u4eec\u83b7\u53d6\u77e5\u8bc6\u3001\u5de5\u4f5c\u548c\u5a31\u4e50\u7684\u65b9\u5f0f\u3002\n\n2. **\u795e\u7ecf\u7f51\u7edc\u4e0e\u6df1\u5ea6\u5b66\u4e60**\uff1a\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u5b66\u4e60\u5c06\u53d8\u5f97\u66f4\u52a0\u5f3a\u5927\u548c\u901a\u7528\uff0c\u80fd\u591f\u5904\u7406\u66f4\u591a\u6837\u5316\u7684\u95ee\u9898\u548c\u6570\u636e\u3002\u4f8b\u5982\uff0c\u5728\u533b\u7597\u8bca\u65ad\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u667a\u80fd\u5236\u9020\u7b49\u9886\u57df\uff0cAI\u53ef\u4ee5\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\n\n3. **\u66f4\u5f3a\u7684\u8ba1\u7b97\u80fd\u529b**\uff1aAI\u5c06\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u3001\u66f4\u5927\u89c4\u6a21\u7684\u6570\u636e\u3002\u8fd9\u5c06\u63a8\u52a8\u8bb8\u591a\u884c\u4e1a\uff0c\u5982\u91d1\u878d\u3001\u533b\u7597\u548c\u79d1\u5b66\u7814\u7a76\uff0c\u4ece\u4f20\u7edf\u7684\u4eba\u5de5\u667a\u80fd\u8f6c\u578b\u5230AI\u9a71\u52a8\u7684\u65b0\u6280\u672f\u3002\n\n4. **\u66f4\u81ea\u7136\u7684\u4ea4\u4e92**\uff1aAI\u5c06\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u6a21\u62df\u4eba\u7c7b\u7684\u81ea\u7136\u8bed\u8a00\u548c\u884c\u4e3a\uff0c\u4f7f\u4eba\u7c7b\u4e0eAI\u80fd\u591f\u66f4\u81ea\u7136\u3001\u66f4\u6d41\u7545\u5730\u4ea4\u6d41\u3002\u8fd9\u5c06\u4f7f\u4eba\u7c7b\u548cAI\u4e4b\u95f4\u7684\u4e92\u52a8\u66f4\u52a0\u65e0\u7f1d\u3002\n\n5. **\u4f26\u7406\u548c\u6cd5\u5f8b**\uff1a\u968f\u7740AI\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f26\u7406\u548c\u6cd5\u5f8b\u95ee\u9898\u5c06\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u6211\u4eec\u9700\u8981\u5236\u5b9a\u660e\u786e\u7684AI\u4f26\u7406\u51c6\u5219\uff0c\u786e\u4fddAI\u6280\u672f\u7684\u4f7f\u7528\u7b26\u5408\u9053\u5fb7\u89c4\u8303\u3002\u8fd9\u9700\u8981\u8de8\n</pre>"},{"location":"howto/messages-truncation/#messages-truncation","title":"Messages Truncation\u00b6","text":"<p>Sometimes LLM services may have limited capacity to handle long messages, which can result in 400 status code errors. Therefore, we need to implement message truncation to keep message lengths within the LLM service's capabilities.</p> <p>The <code>tablegpt-agent</code> provides a <code>TruncationConfig</code> class to specify truncation settings for the LLM and VLM.</p>"},{"location":"howto/messages-truncation/#too-long-messages-without-truncation","title":"Too long messages without truncation\u00b6","text":""},{"location":"howto/messages-truncation/#too-long-messages-with-truncation","title":"Too long messages with truncation\u00b6","text":""},{"location":"howto/messages-truncation/#truncationconfig-settings-in-create_tablegpt_graph","title":"TruncationConfig settings in <code>create_tablegpt_graph</code>\u00b6","text":"<ul> <li><code>llm_truncation_config</code>: Truncate messages sent to pure language models</li> <li><code>vlm_truncation_config</code>: Truncate messages sent to vision+language multimodal models</li> </ul>"},{"location":"howto/messages-truncation/#create-truncationconfig","title":"Create TruncationConfig\u00b6","text":"<p>The parameters set in <code>TruncationConfig</code> will be used in <code>langchain_core.messages.trim_messages</code>, see trim_messages documentation</p>"},{"location":"howto/normalize-datasets/","title":"Normalize Datasets","text":"<p>!!! Note: When the <code>tablegpt-agent</code> enables the <code>Dataset Normalizer</code> to format the dataset, the dataset reading process will be noticeably slower. This is because the <code>Dataset Normalizer</code> needs to analyze the dataset and generate transformation code, a process that takes considerable time.</p> <p>It is worth noting that the data normalization process can effectively address most common data irregularities. However, for more complex datasets, further optimization may be needed, and the results depend on the specific normalization model used.</p> In\u00a0[7]: Copied! <pre>from pathlib import Path\nfrom langchain_openai import ChatOpenAI\nfrom pybox import AsyncLocalPyBoxManager\nfrom tablegpt.agent import create_tablegpt_graph\nfrom tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\nnormalize_llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"YOUR_VLLM_MODEL_NAME\")\npybox_manager = AsyncLocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\n\nagent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    normalize_llm=normalize_llm,\n    session_id=\"some-session-id\", # This is required when using file-reading\n)\n</pre> from pathlib import Path from langchain_openai import ChatOpenAI from pybox import AsyncLocalPyBoxManager from tablegpt.agent import create_tablegpt_graph from tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") normalize_llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"YOUR_VLLM_MODEL_NAME\") pybox_manager = AsyncLocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)  agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     normalize_llm=normalize_llm,     session_id=\"some-session-id\", # This is required when using file-reading ) <p>Given an Excel file \u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx with merged cells and irregular headers:</p> \u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868 \u751f\u4ea7\u65e5\u671f \u5236\u9020\u7f16\u53f7 \u4ea7\u54c1\u540d\u79f0 \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf \u7d2f\u8ba1\u4ea7\u91cf \u8017\u8d39\u5de5\u65f6 \u9884\u8ba1 \u5b9e\u9645 \u672c\u65e5 \u7d2f\u8ba1 2007/8/10 FK-001 \u7315\u7334\u6843\u679c\u8089\u996e\u6599 100000 40000 45000 83000 10 20 2007/8/11 FK-002 \u897f\u74dc\u679c\u8089\u996e\u6599 100000 40000 44000 82000 9 18 2007/8/12 FK-003 \u8349\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 2007/8/13 FK-004 \u84dd\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 <p>Add the file for processing in the <code>additional_kwargs</code> of HumanMessage:</p> In\u00a0[8]: Copied! <pre>from typing import TypedDict\nfrom langchain_core.messages import HumanMessage\n\nclass Attachment(TypedDict):\n    \"\"\"Contains at least one dictionary with the key filename.\"\"\"\n    filename: str\n\nattachment_msg = HumanMessage(\n    content=\"\",\n    # Please make sure your iPython kernel can access your filename.\n    additional_kwargs={\"attachments\": [Attachment(filename=\"\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx\")]},\n)\n</pre> from typing import TypedDict from langchain_core.messages import HumanMessage  class Attachment(TypedDict):     \"\"\"Contains at least one dictionary with the key filename.\"\"\"     filename: str  attachment_msg = HumanMessage(     content=\"\",     # Please make sure your iPython kernel can access your filename.     additional_kwargs={\"attachments\": [Attachment(filename=\"\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx\")]}, ) <p>Invoke the <code>tablegpt-agent</code> to normalize the datasets:</p> In\u00a0[9]: Copied! <pre>from datetime import date\nfrom tablegpt.agent.file_reading import Stage\n\n# Reading and processing files.\nresponse = await agent.ainvoke(\n    input={\n        \"entry_message\": attachment_msg,\n        \"processing_stage\": Stage.UPLOADED,\n        \"messages\": [attachment_msg],\n        \"parent_id\": \"some-parent-id1\",\n        \"date\": date.today(),\n    },\n    config={\n        # Using checkpointer requires binding thread_id at runtime.\n        \"configurable\": {\"thread_id\": \"some-thread-id\"},\n    },\n)\n\nresponse[\"messages\"]\n</pre> from datetime import date from tablegpt.agent.file_reading import Stage  # Reading and processing files. response = await agent.ainvoke(     input={         \"entry_message\": attachment_msg,         \"processing_stage\": Stage.UPLOADED,         \"messages\": [attachment_msg],         \"parent_id\": \"some-parent-id1\",         \"date\": date.today(),     },     config={         # Using checkpointer requires binding thread_id at runtime.         \"configurable\": {\"thread_id\": \"some-thread-id\"},     }, )  response[\"messages\"] Out[9]: <pre>[HumanMessage(content='', additional_kwargs={'attachments': [{'filename': '\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx'}]}, response_metadata={}, id='452ef08b-a45f-49ce-8489-4062cd35d57c'),\n AIMessage(content='\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u6570\u636e\u6587\u4ef6\uff0c\u6211\u9700\u8981\u67e5\u770b\u6587\u4ef6\u5185\u5bb9\u4ee5\u5bf9\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u521d\u6b65\u7684\u4e86\u89e3\u3002\u9996\u5148\u6211\u4f1a\u8bfb\u53d6\u6570\u636e\u5230 `df` \u53d8\u91cf\u4e2d\uff0c\u5e76\u901a\u8fc7 `df.info` \u67e5\u770b NaN \u60c5\u51b5\u548c\u6570\u636e\u7c7b\u578b\u3002\\n```python\\n# Load the data into a DataFrame\\ndf = read_df(\\'\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx\\')\\n# Normalize the data\\ntry:\\n    df = df.copy()\\n\\n    import pandas as pd\\n\\n    # Assuming the original data is loaded into a DataFrame named `df`\\n\\n    # Step 1: Isolate the Table Header\\n    # Remove the unnecessary top rows and columns\\n    df = df.drop([0, 1])  # Drop the first two rows which contain header information\\n    df = df.reset_index(drop=True)  # Reset the index to start from 0\\n\\n    # Step 2: Store the Result as `final_df`\\n    final_df = df\\n\\n    # Step 3: Rename Columns of final_df\\n    # Adjust the column names of final_df to match the desired format\\n    final_df.columns = [\\'\u751f\u4ea7\u65e5\u671f\\', \\'\u5236\u9020\u7f16\u53f7\\', \\'\u4ea7\u54c1\u540d\u79f0\\', \\'\u9884\u5b9a\u4ea7\u91cf\\', \\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\', \\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\', \\'\u7d2f\u8ba1\u4ea7\u91cf\\', \\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\', \\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\']\\n\\n    # Step 4: Data Processing\\n    # Ensure the data types are correct and remove any unnecessary rows or columns\\n    final_df = final_df.dropna(subset=[\\'\u751f\u4ea7\u65e5\u671f\\'])  # Remove rows where the production date is missing\\n    final_df = final_df.drop_duplicates()  # Remove duplicate rows if any\\n    final_df = final_df.reset_index(drop=True)  # Reset the index again to ensure it starts from 0\\n\\n    # Convert columns to appropriate data types\\n    final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'] = final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'] = final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n\\n    # Display the transformed DataFrame\\n    print(final_df)\\n    if final_df.columns.tolist() == final_df.iloc[0].tolist():\\n        final_df = final_df.iloc[1:]\\n\\n    # reassign df with the formatted DataFrame\\n    df = final_df\\nexcept Exception as e:\\n    # Unable to apply formatting to the original DataFrame. proceeding with the unformatted DataFrame.\\n    print(f\"Reformat failed with error {e}, use the original DataFrame.\")\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how=\\'all\\').dropna(axis=1, how=\\'all\\')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\\n```', additional_kwargs={'parent_id': 'some-parent-id1', 'thought': '\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u6570\u636e\u6587\u4ef6\uff0c\u6211\u9700\u8981\u67e5\u770b\u6587\u4ef6\u5185\u5bb9\u4ee5\u5bf9\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u521d\u6b65\u7684\u4e86\u89e3\u3002\u9996\u5148\u6211\u4f1a\u8bfb\u53d6\u6570\u636e\u5230 `df` \u53d8\u91cf\u4e2d\uff0c\u5e76\u901a\u8fc7 `df.info` \u67e5\u770b NaN \u60c5\u51b5\u548c\u6570\u636e\u7c7b\u578b\u3002', 'action': {'tool': 'python', 'tool_input': '# Load the data into a DataFrame\\ndf = read_df(\\'\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx\\')\\n# Normalize the data\\ntry:\\n    df = df.copy()\\n\\n    import pandas as pd\\n\\n    # Assuming the original data is loaded into a DataFrame named `df`\\n\\n    # Step 1: Isolate the Table Header\\n    # Remove the unnecessary top rows and columns\\n    df = df.drop([0, 1])  # Drop the first two rows which contain header information\\n    df = df.reset_index(drop=True)  # Reset the index to start from 0\\n\\n    # Step 2: Store the Result as `final_df`\\n    final_df = df\\n\\n    # Step 3: Rename Columns of final_df\\n    # Adjust the column names of final_df to match the desired format\\n    final_df.columns = [\\'\u751f\u4ea7\u65e5\u671f\\', \\'\u5236\u9020\u7f16\u53f7\\', \\'\u4ea7\u54c1\u540d\u79f0\\', \\'\u9884\u5b9a\u4ea7\u91cf\\', \\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\', \\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\', \\'\u7d2f\u8ba1\u4ea7\u91cf\\', \\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\', \\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\']\\n\\n    # Step 4: Data Processing\\n    # Ensure the data types are correct and remove any unnecessary rows or columns\\n    final_df = final_df.dropna(subset=[\\'\u751f\u4ea7\u65e5\u671f\\'])  # Remove rows where the production date is missing\\n    final_df = final_df.drop_duplicates()  # Remove duplicate rows if any\\n    final_df = final_df.reset_index(drop=True)  # Reset the index again to ensure it starts from 0\\n\\n    # Convert columns to appropriate data types\\n    final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'] = final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'] = final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n\\n    # Display the transformed DataFrame\\n    print(final_df)\\n    if final_df.columns.tolist() == final_df.iloc[0].tolist():\\n        final_df = final_df.iloc[1:]\\n\\n    # reassign df with the formatted DataFrame\\n    df = final_df\\nexcept Exception as e:\\n    # Unable to apply formatting to the original DataFrame. proceeding with the unformatted DataFrame.\\n    print(f\"Reformat failed with error {e}, use the original DataFrame.\")\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how=\\'all\\').dropna(axis=1, how=\\'all\\')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)'}, 'model_type': None}, response_metadata={}, id='8e5d0026-215d-46e2-ab60-9174c5bf50bd', tool_calls=[{'name': 'python', 'args': {'query': '# Load the data into a DataFrame\\ndf = read_df(\\'\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx\\')\\n# Normalize the data\\ntry:\\n    df = df.copy()\\n\\n    import pandas as pd\\n\\n    # Assuming the original data is loaded into a DataFrame named `df`\\n\\n    # Step 1: Isolate the Table Header\\n    # Remove the unnecessary top rows and columns\\n    df = df.drop([0, 1])  # Drop the first two rows which contain header information\\n    df = df.reset_index(drop=True)  # Reset the index to start from 0\\n\\n    # Step 2: Store the Result as `final_df`\\n    final_df = df\\n\\n    # Step 3: Rename Columns of final_df\\n    # Adjust the column names of final_df to match the desired format\\n    final_df.columns = [\\'\u751f\u4ea7\u65e5\u671f\\', \\'\u5236\u9020\u7f16\u53f7\\', \\'\u4ea7\u54c1\u540d\u79f0\\', \\'\u9884\u5b9a\u4ea7\u91cf\\', \\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\', \\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\', \\'\u7d2f\u8ba1\u4ea7\u91cf\\', \\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\', \\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\']\\n\\n    # Step 4: Data Processing\\n    # Ensure the data types are correct and remove any unnecessary rows or columns\\n    final_df = final_df.dropna(subset=[\\'\u751f\u4ea7\u65e5\u671f\\'])  # Remove rows where the production date is missing\\n    final_df = final_df.drop_duplicates()  # Remove duplicate rows if any\\n    final_df = final_df.reset_index(drop=True)  # Reset the index again to ensure it starts from 0\\n\\n    # Convert columns to appropriate data types\\n    final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'] = final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'] = final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n\\n    # Display the transformed DataFrame\\n    print(final_df)\\n    if final_df.columns.tolist() == final_df.iloc[0].tolist():\\n        final_df = final_df.iloc[1:]\\n\\n    # reassign df with the formatted DataFrame\\n    df = final_df\\nexcept Exception as e:\\n    # Unable to apply formatting to the original DataFrame. proceeding with the unformatted DataFrame.\\n    print(f\"Reformat failed with error {e}, use the original DataFrame.\")\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how=\\'all\\').dropna(axis=1, how=\\'all\\')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)'}, 'id': 'af99b549-09f1-4623-930a-3ffbfde40216', 'type': 'tool_call'}]),\n ToolMessage(content=[{'type': 'text', 'text': \"```pycon\\nReformat failed with error cannot convert float NaN to integer, use the original DataFrame.\\n&lt;class 'pandas.core.frame.DataFrame'&gt;\\nRangeIndex: 26 entries, 0 to 25\\nData columns (total 9 columns):\\n #   Column  Non-Null Count  Dtype \\n---  ------  --------------  ----- \\n 0   \u751f\u4ea7\u65e5\u671f    22 non-null     object\\n 1   \u5236\u9020\u7f16\u53f7    25 non-null     object\\n 2   \u4ea7\u54c1\u540d\u79f0    20 non-null     object\\n 3   \u9884\u5b9a\u4ea7\u91cf    20 non-null     object\\n 4   \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1  21 non-null     object\\n 5   \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645  21 non-null     object\\n 6   \u7d2f\u8ba1\u4ea7\u91cf    25 non-null     object\\n 7   \u672c\u65e5\u8017\u8d39\u5de5\u65f6  20 non-null     object\\n 8   \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6  20 non-null     object\\ndtypes: object(9)\\n```\"}], name='python', id='58429576-461b-4a50-8c3d-5bbacca22cdb', tool_call_id='af99b549-09f1-4623-930a-3ffbfde40216', artifact=[]),\n AIMessage(content='\u63a5\u4e0b\u6765\u6211\u5c06\u7528 `df.head(5)` \u6765\u67e5\u770b\u6570\u636e\u96c6\u7684\u524d 5 \u884c\u3002\\n```python\\n# Show the first 5 rows to understand the structure\\ndf.head(5)\\n```', additional_kwargs={'parent_id': 'some-parent-id1', 'thought': '\u63a5\u4e0b\u6765\u6211\u5c06\u7528 `df.head(5)` \u6765\u67e5\u770b\u6570\u636e\u96c6\u7684\u524d 5 \u884c\u3002', 'action': {'tool': 'python', 'tool_input': '# Show the first 5 rows to understand the structure\\ndf.head(5)'}, 'model_type': None}, response_metadata={}, id='0c67c2b2-36d2-49cc-8fb8-7f6e5dfc8625', tool_calls=[{'name': 'python', 'args': {'query': '# Show the first 5 rows to understand the structure\\ndf.head(5)'}, 'id': 'a4ccd41a-a872-4ff0-aae5-678a96b9b54a', 'type': 'tool_call'}]),\n ToolMessage(content=[{'type': 'text', 'text': '```pycon\\n                  \u751f\u4ea7\u65e5\u671f    \u5236\u9020\u7f16\u53f7     \u4ea7\u54c1\u540d\u79f0    \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1 \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645   \u7d2f\u8ba1\u4ea7\u91cf \u672c\u65e5\u8017\u8d39\u5de5\u65f6 \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\n0  2007-08-10 00:00:00  FK-001  \u7315\u7334\u6843\u679c\u8089\u996e\u6599  100000  40000  45000  83000     10     20\\n1  2007-08-11 00:00:00  FK-002   \u897f\u74dc\u679c\u8089\u996e\u6599  100000  40000  44000  82000      9     18\\n2  2007-08-12 00:00:00  FK-003   \u8349\u8393\u679c\u8089\u996e\u6599  100000  40000  45000  83000      9     18\\n3  2007-08-13 00:00:00  FK-004   \u84dd\u8393\u679c\u8089\u996e\u6599  100000  40000  45000  83000      9     18\\n4  2007-08-14 00:00:00  FK-005  \u6c34\u5bc6\u6843\u679c\u8089\u996e\u6599  100000  40000  45000  83000     10     20\\n```'}], name='python', id='d828aa34-7c9e-4fee-8ae1-7b553530292b', tool_call_id='a4ccd41a-a872-4ff0-aae5-678a96b9b54a', artifact=[]),\n AIMessage(content='\u6211\u5df2\u7ecf\u4e86\u89e3\u4e86\u6570\u636e\u96c6 \u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx \u7684\u57fa\u672c\u4fe1\u606f\u3002\u8bf7\u95ee\u6211\u53ef\u4ee5\u5e2e\u60a8\u505a\u4e9b\u4ec0\u4e48\uff1f', additional_kwargs={'parent_id': 'some-parent-id1'}, response_metadata={}, id='e836eba6-9597-4bf8-acfd-2a81871916a6')]</pre> <p>By formatting the content of the last <code>ToolMessage</code>, you can see the normalized data:</p> \u751f\u4ea7\u65e5\u671f \u5236\u9020\u7f16\u53f7 \u4ea7\u54c1\u540d\u79f0 \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1 \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645 \u7d2f\u8ba1\u4ea7\u91cf \u672c\u65e5\u8017\u8d39\u5de5\u65f6 \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6 2007/8/10 FK-001 \u7315\u7334\u6843\u679c\u8089\u996e\u6599 100000 40000 45000 83000 10 20 2007/8/11 FK-002 \u897f\u74dc\u679c\u8089\u996e\u6599 100000 40000 44000 82000 9 18 2007/8/12 FK-003 \u8349\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 2007/8/13 FK-004 \u84dd\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18"},{"location":"howto/normalize-datasets/#normalize-datasets","title":"Normalize Datasets\u00b6","text":"<p>The Dataset Normalizer plugin is used to transform 'pandas-unfriendly' datasets (e.g., Excel files that do not follow a standard tabular structure) into a more suitable format for pandas. It is backed by an LLM that generates Python code to convert the original datasets into new ones.</p> <p>In <code>tablegpt-agent</code>, this plugin is used to better format 'pandas-unfriendly' datasets, making them more understandable for the subsequent steps. This plugin is optional; if used, it serves as the very first step in the File Reading Workflow, easing the difficulty of data analysis in the subsequent workflow.</p>"},{"location":"howto/normalize-datasets/#introduction","title":"Introduction\u00b6","text":"<p>The <code>Dataset Normalizer</code> is a specialized tool designed to tackle challenges that arise when working with irregular and poorly structured datasets. These challenges are especially prevalent in Excel files, which are often used as a flexible but inconsistent way of storing data.</p> <p>Analyzing Excel data files can pose significant challenges, such as:</p> <ul> <li>Irregular Formatting: Datasets may lack a consistent tabular structure, with varying cell sizes or non-standard layouts.</li> <li>Merged Cells: Cells spanning multiple rows or columns can disrupt parsing tools.</li> <li>Inconsistent Headers: Columns may have incomplete, redundant, or nested headers.</li> <li>Hidden Data: Data may be stored in additional sheets or rely on calculated fields that are not directly accessible.</li> <li>Mixed Data Types: Columns may contain inconsistent data types, such as numbers mixed with text.</li> <li>Empty or Placeholder Rows: Extra rows with missing or irrelevant data can complicate data loading and analysis.</li> </ul>"},{"location":"howto/normalize-datasets/#quick-start","title":"Quick Start\u00b6","text":"<p>To enable the <code>Dataset Normalizer</code>, ensure you pass it as a parameter when creating the <code>tablegpt-agent</code>. You can follow the example below:</p>"},{"location":"howto/persist-messages/","title":"Persist Messages","text":"In\u00a0[\u00a0]: Copied! <pre>from datetime import date\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom tablegpt.agent import create_tablegpt_graph\nfrom pybox import AsyncLocalPyBoxManager\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\npybox_manager = AsyncLocalPyBoxManager()\ncheckpointer = MemorySaver()\n\ngraph = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    checkpointer=checkpointer,\n)\n</pre> from datetime import date  from langchain_openai import ChatOpenAI from langgraph.checkpoint.memory import MemorySaver from tablegpt.agent import create_tablegpt_graph from pybox import AsyncLocalPyBoxManager  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") pybox_manager = AsyncLocalPyBoxManager() checkpointer = MemorySaver()  graph = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     checkpointer=checkpointer, ) <p>Conducting a Conversation with <code>TableGPT</code></p> In\u00a0[2]: Copied! <pre>resp = await graph.ainvoke(\n    input={\n        \"messages\": [(\"human\", \"Please introduce Jackie Chan\")],\n        \"parent_id\": \"1\",\n        \"date\": date.today(),\n    },\n    config={\"configurable\": {\"thread_id\": \"1\"}},\n)\nresp[\"messages\"][-1]\n</pre> resp = await graph.ainvoke(     input={         \"messages\": [(\"human\", \"Please introduce Jackie Chan\")],         \"parent_id\": \"1\",         \"date\": date.today(),     },     config={\"configurable\": {\"thread_id\": \"1\"}}, ) resp[\"messages\"][-1] Out[2]: <pre>AIMessage(content=\"I understand that you're asking for an introduction to Jackie Chan. However, my primary role is to analyze datasets using Python. If you have a dataset related to Jackie Chan or any other topic, I'd be happy to help you analyze it. Could you please provide more details on what kind of data you have or what specific analysis you would like to perform?\", additional_kwargs={'parent_id': '1'}, response_metadata={}, id='cdf638ce-0e56-475b-a86b-0d8d7a0f6d05')</pre> <p>Continuing the Conversation</p> <p>To extend the conversation while maintaining context, you can provide new input along with the same <code>config</code> configuration:</p> <p>Note: <code>config</code> is the configuration associated with this <code>checkpointer</code>. Through this configuration, the <code>checkpointer</code> can retrieve previous status information, so that in subsequent conversations, the model can better understand the user's intention and reply.</p> In\u00a0[3]: Copied! <pre>resp = await graph.ainvoke(\n    input={\n        \"messages\": [(\"human\", \"Please name three movies he participated in.\")],\n        \"parent_id\": \"1\",\n        \"date\": date.today(),\n    },\n    config={\"configurable\": {\"thread_id\": \"1\"}},\n)\nresp[\"messages\"][-1]\n</pre> resp = await graph.ainvoke(     input={         \"messages\": [(\"human\", \"Please name three movies he participated in.\")],         \"parent_id\": \"1\",         \"date\": date.today(),     },     config={\"configurable\": {\"thread_id\": \"1\"}}, ) resp[\"messages\"][-1] Out[3]: <pre>AIMessage(content=\"Certainly! Jackie Chan is a renowned actor, director, and martial artist, and he has starred in numerous films. Here are three popular movies in which he has participated:\\n\\n1. **Rush Hour (1998)** - In this action-comedy film, Jackie Chan plays the role of Inspector Lee, a Hong Kong detective who teams up with a Los Angeles detective, played by Chris Tucker, to solve a kidnapping case.\\n\\n2. **Drunken Master (1978)** - This is one of Jackie Chan's early films where he plays a young man who learns the art of drunken boxing to avenge his father's enemies.\\n\\n3. **The Karate Kid (2010)** - In this remake of the original 1984 film, Jackie Chan plays Mr. Han, a maintenance man who becomes the mentor to a young boy, Jaden Smith, teaching him martial arts and life lessons.\\n\\nIf you have any specific data or analysis related to these movies or Jackie Chan's filmography, feel free to provide more details, and I can help you with that!\", additional_kwargs={'parent_id': '1'}, response_metadata={}, id='4f62bec2-a4ec-43ad-97b2-cbade8c774b4')</pre> <p>Next, we demonstrates how to use <code>Postgres</code> as the backend for persisting checkpoint state using the langgraph-checkpoint-postgres library.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install -U psycopg psycopg-pool psycopg_binary langgraph langgraph-checkpoint-postgres\n</pre> %pip install -U psycopg psycopg-pool psycopg_binary langgraph langgraph-checkpoint-postgres In\u00a0[4]: Copied! <pre>DB_URI = \"postgresql://postgres:postgres@127.0.0.1:5432/postgres?sslmode=disable\"\n</pre> DB_URI = \"postgresql://postgres:postgres@127.0.0.1:5432/postgres?sslmode=disable\" In\u00a0[5]: Copied! <pre>from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\nfrom tablegpt.agent import create_tablegpt_graph\n\nconfig = {\"configurable\": {\"thread_id\": \"2\"}}\n\nasync with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    graph = create_tablegpt_graph(\n        llm=llm,\n        pybox_manager=pybox_manager,\n        checkpointer=checkpointer,\n    )\n    \n    res = await graph.ainvoke(\n        input={\n            \"messages\": [(\"human\", \"Who are you?\")],\n            \"parent_id\": \"2\",\n            \"date\": date.today()\n        },\n        config=config,\n    )\n    checkpoint_tuples = [c async for c in checkpointer.alist(config)]\n</pre> from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver from tablegpt.agent import create_tablegpt_graph  config = {\"configurable\": {\"thread_id\": \"2\"}}  async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:     graph = create_tablegpt_graph(         llm=llm,         pybox_manager=pybox_manager,         checkpointer=checkpointer,     )          res = await graph.ainvoke(         input={             \"messages\": [(\"human\", \"Who are you?\")],             \"parent_id\": \"2\",             \"date\": date.today()         },         config=config,     )     checkpoint_tuples = [c async for c in checkpointer.alist(config)] In\u00a0[6]: Copied! <pre>checkpoint_tuples\n</pre> checkpoint_tuples Out[6]: <pre>[CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-5d51-63c3-8001-9bd42cf9d6e6'}}, checkpoint={'v': 1, 'id': '1efa6543-5d51-63c3-8001-9bd42cf9d6e6', 'ts': '2024-11-19T08:56:48.239486+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.6926057190269731'}, 'data_analyze_graph': {'branch:__start__:router:data_analyze_graph': '00000000000000000000000000000002.0.32407437506283565'}}, 'channel_versions': {'date': '00000000000000000000000000000003.0.1780977977687367', 'messages': '00000000000000000000000000000003.0.05509702188973753', '__start__': '00000000000000000000000000000002.3.0886787893869005e-05', 'parent_id': '00000000000000000000000000000003.0.43858547879187637', 'data_analyze_graph': '00000000000000000000000000000003.0.1082481333441786', 'branch:__start__:router:data_analyze_graph': '00000000000000000000000000000003.0.593567034515958'}, 'channel_values': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')], 'parent_id': '2', 'data_analyze_graph': 'data_analyze_graph'}}, metadata={'step': 1, 'source': 'loop', 'writes': {'data_analyze_graph': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')], 'parent_id': '2'}}, 'parents': {}, 'thread_id': '2'}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}}, pending_writes=[]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-5d27-6b37-8002-4e274906f4a5'}}, checkpoint={'v': 1, 'id': '1efa6543-5d27-6b37-8002-4e274906f4a5', 'ts': '2024-11-19T08:56:48.222457+00:00', 'pending_sends': [], 'versions_seen': {'agent': {'join:input_guard+retrieve_columns:agent': '00000000000000000000000000000003.0.8898909183470118'}, '__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.6163867462467301'}, 'input_guard': {'start:input_guard': '00000000000000000000000000000002.0.6848611387807798'}, 'retrieve_columns': {'start:retrieve_columns': '00000000000000000000000000000002.0.030416452982199194'}}, 'channel_versions': {'date': '00000000000000000000000000000002.0.2490273362085793', 'agent': '00000000000000000000000000000005.0.024232530645486583', 'messages': '00000000000000000000000000000005.0.14282746367420773', '__start__': '00000000000000000000000000000002.0.18620036399153372', 'parent_id': '00000000000000000000000000000002.0.5201095646733788', 'input_guard': '00000000000000000000000000000005.0.859473129275239', 'retrieve_columns': '00000000000000000000000000000005.0.7752176585300508', 'start:input_guard': '00000000000000000000000000000003.0.6183120254220215', 'start:retrieve_columns': '00000000000000000000000000000003.0.08187600687354024', 'join:input_guard+retrieve_columns:agent': '00000000000000000000000000000004.0.5107824581933167'}, 'channel_values': {'date': datetime.date(2024, 11, 19), 'agent': 'agent', 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')], 'parent_id': '2', 'join:input_guard+retrieve_columns:agent': set()}}, metadata={'step': 2, 'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')]}}, 'parents': {'': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}, 'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'langgraph_node': 'data_analyze_graph', 'langgraph_path': ['__pregel_pull', 'data_analyze_graph'], 'langgraph_step': 1, 'langgraph_triggers': ['branch:__start__:router:data_analyze_graph'], 'langgraph_checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd'}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a53-6aa4-8001-7474db32528e'}}, pending_writes=[]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a53-6aa4-8001-7474db32528e'}}, checkpoint={'v': 1, 'id': '1efa6543-4a53-6aa4-8001-7474db32528e', 'ts': '2024-11-19T08:56:46.248182+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.6163867462467301'}, 'input_guard': {'start:input_guard': '00000000000000000000000000000002.0.6848611387807798'}, 'retrieve_columns': {'start:retrieve_columns': '00000000000000000000000000000002.0.030416452982199194'}}, 'channel_versions': {'date': '00000000000000000000000000000002.0.2490273362085793', 'messages': '00000000000000000000000000000003.0.8478737633204881', '__start__': '00000000000000000000000000000002.0.18620036399153372', 'parent_id': '00000000000000000000000000000002.0.5201095646733788', 'input_guard': '00000000000000000000000000000003.0.06039244147872136', 'retrieve_columns': '00000000000000000000000000000003.0.8552403538042089', 'start:input_guard': '00000000000000000000000000000003.0.6183120254220215', 'start:retrieve_columns': '00000000000000000000000000000003.0.08187600687354024', 'join:input_guard+retrieve_columns:agent': '00000000000000000000000000000003.0.8898909183470118'}, 'channel_values': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')], 'parent_id': '2', 'input_guard': 'input_guard', 'retrieve_columns': 'retrieve_columns', 'join:input_guard+retrieve_columns:agent': {'input_guard', 'retrieve_columns'}}}, metadata={'step': 1, 'source': 'loop', 'writes': {'input_guard': {'messages': []}, 'retrieve_columns': {'messages': []}}, 'parents': {'': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}, 'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'langgraph_node': 'data_analyze_graph', 'langgraph_path': ['__pregel_pull', 'data_analyze_graph'], 'langgraph_step': 1, 'langgraph_triggers': ['branch:__start__:router:data_analyze_graph'], 'langgraph_checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd'}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a4c-6639-8000-a5baf4d38bd2'}}, pending_writes=[('36d7d700-5338-feda-05f0-57d74fddbc0b', 'agent', 'agent'), ('36d7d700-5338-feda-05f0-57d74fddbc0b', 'messages', [AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')])]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a4c-6639-8000-a5baf4d38bd2'}}, checkpoint={'v': 1, 'id': '1efa6543-4a4c-6639-8000-a5baf4d38bd2', 'ts': '2024-11-19T08:56:46.245208+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.6163867462467301'}}, 'channel_versions': {'date': '00000000000000000000000000000002.0.2490273362085793', 'messages': '00000000000000000000000000000002.0.19507603965774079', '__start__': '00000000000000000000000000000002.0.18620036399153372', 'parent_id': '00000000000000000000000000000002.0.5201095646733788', 'start:input_guard': '00000000000000000000000000000002.0.6848611387807798', 'start:retrieve_columns': '00000000000000000000000000000002.0.030416452982199194'}, 'channel_values': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')], 'parent_id': '2', 'start:input_guard': '__start__', 'start:retrieve_columns': '__start__'}}, metadata={'step': 0, 'source': 'loop', 'writes': None, 'parents': {'': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}, 'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'langgraph_node': 'data_analyze_graph', 'langgraph_path': ['__pregel_pull', 'data_analyze_graph'], 'langgraph_step': 1, 'langgraph_triggers': ['branch:__start__:router:data_analyze_graph'], 'langgraph_checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd'}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a49-6ec0-bfff-7a6bdf830d6b'}}, pending_writes=[('637b8d5c-1e9a-d15c-7560-eba440c88860', 'input_guard', 'input_guard'), ('637b8d5c-1e9a-d15c-7560-eba440c88860', 'messages', []), ('637b8d5c-1e9a-d15c-7560-eba440c88860', 'join:input_guard+retrieve_columns:agent', 'input_guard'), ('fcc182eb-567e-cef1-c5be-16b527e21434', 'retrieve_columns', 'retrieve_columns'), ('fcc182eb-567e-cef1-c5be-16b527e21434', 'messages', []), ('fcc182eb-567e-cef1-c5be-16b527e21434', 'join:input_guard+retrieve_columns:agent', 'retrieve_columns')]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a49-6ec0-bfff-7a6bdf830d6b'}}, checkpoint={'v': 1, 'id': '1efa6543-4a49-6ec0-bfff-7a6bdf830d6b', 'ts': '2024-11-19T08:56:46.244206+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}}, 'channel_versions': {'__start__': '00000000000000000000000000000001.0.6163867462467301'}, 'channel_values': {'__start__': {'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')], 'parent_id': '2', 'date': datetime.date(2024, 11, 19)}}}, metadata={'step': -1, 'source': 'input', 'writes': {'__start__': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')], 'parent_id': '2'}}, 'parents': {'': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}, 'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'langgraph_node': 'data_analyze_graph', 'langgraph_path': ['__pregel_pull', 'data_analyze_graph'], 'langgraph_step': 1, 'langgraph_triggers': ['branch:__start__:router:data_analyze_graph'], 'langgraph_checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd'}, parent_config=None, pending_writes=[('39da96de-984e-3d02-e2ef-9bd5146d7336', 'messages', [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')]), ('39da96de-984e-3d02-e2ef-9bd5146d7336', 'date', datetime.date(2024, 11, 19)), ('39da96de-984e-3d02-e2ef-9bd5146d7336', 'parent_id', '2'), ('39da96de-984e-3d02-e2ef-9bd5146d7336', 'start:input_guard', '__start__'), ('39da96de-984e-3d02-e2ef-9bd5146d7336', 'start:retrieve_columns', '__start__')]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}}, checkpoint={'v': 1, 'id': '1efa6543-4a1a-6852-8000-b975c65fb2ff', 'ts': '2024-11-19T08:56:46.224784+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.6926057190269731'}}, 'channel_versions': {'date': '00000000000000000000000000000002.0.602279509708772', 'messages': '00000000000000000000000000000002.0.47212683047327253', '__start__': '00000000000000000000000000000002.3.0886787893869005e-05', 'parent_id': '00000000000000000000000000000002.0.43326965052986344', 'branch:__start__:router:data_analyze_graph': '00000000000000000000000000000002.0.32407437506283565'}, 'channel_values': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')], 'parent_id': '2', 'branch:__start__:router:data_analyze_graph': '__start__'}}, metadata={'step': 0, 'source': 'loop', 'writes': None, 'parents': {}, 'thread_id': '2'}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-4a12-6bbb-bfff-ac4aee846bfe'}}, pending_writes=[('3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'data_analyze_graph', 'data_analyze_graph'), ('3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'messages', [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')]), ('3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'parent_id', '2'), ('3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'date', datetime.date(2024, 11, 19))]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-4a12-6bbb-bfff-ac4aee846bfe'}}, checkpoint={'v': 1, 'id': '1efa6543-4a12-6bbb-bfff-ac4aee846bfe', 'ts': '2024-11-19T08:56:46.221598+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}}, 'channel_versions': {'__start__': '00000000000000000000000000000001.0.6926057190269731'}, 'channel_values': {'__start__': {'messages': [['human', 'Who are you?']], 'parent_id': '2', 'date': datetime.date(2024, 11, 19)}}}, metadata={'step': -1, 'source': 'input', 'writes': {'__start__': {'date': datetime.date(2024, 11, 19), 'messages': [['human', 'Who are you?']], 'parent_id': '2'}}, 'parents': {}, 'thread_id': '2'}, parent_config=None, pending_writes=[('8e27b3ac-0a9d-697e-0667-6d3ccc170a50', 'messages', [['human', 'Who are you?']]), ('8e27b3ac-0a9d-697e-0667-6d3ccc170a50', 'parent_id', '2'), ('8e27b3ac-0a9d-697e-0667-6d3ccc170a50', 'date', datetime.date(2024, 11, 19)), ('8e27b3ac-0a9d-697e-0667-6d3ccc170a50', 'branch:__start__:router:data_analyze_graph', '__start__')])]</pre> In\u00a0[7]: Copied! <pre>async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    graph = create_tablegpt_graph(\n        llm=llm,\n        pybox_manager=pybox_manager,\n        checkpointer=checkpointer,\n    )\n    \n    graph_state = await graph.aget_state(config)\n</pre> async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:     graph = create_tablegpt_graph(         llm=llm,         pybox_manager=pybox_manager,         checkpointer=checkpointer,     )          graph_state = await graph.aget_state(config) In\u00a0[8]: Copied! <pre>graph_state\n</pre> graph_state Out[8]: <pre>StateSnapshot(values={'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')], 'parent_id': '2', 'date': datetime.date(2024, 11, 19)}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-5d51-63c3-8001-9bd42cf9d6e6'}}, metadata={'step': 1, 'source': 'loop', 'writes': {'data_analyze_graph': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')], 'parent_id': '2'}}, 'parents': {}, 'thread_id': '2'}, created_at='2024-11-19T08:56:48.239486+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}}, tasks=())</pre>"},{"location":"howto/persist-messages/#persist-messages","title":"Persist Messages\u00b6","text":"<p>When creating TableGPT agents, you have the option to persist their state, enabling interactions with the agent across multiple sessions while retaining memory of previous interactions. For more information on persistence, you can refer to the Persistence documentation.</p> <p>The benefit of persistent messages is that you can interact with the TableGPT agent across multiple sessions, and the agent remembers previous interactions. This is useful for applications that require long-term tracking of context or complex conversations.</p> <p>TableGPT Agent leverages langgraph-checkpoint to implement persistent message storage. It supports using any type of <code>checkpointer</code> to store messages, such as: <code>Postgres</code>, <code>Redis</code>, <code>Memory</code>, etc. To integrate a checkpointer with a <code>TableGPT Agent</code>, you can follow the example below:</p>"},{"location":"howto/persist-messages/#installing-required-packages","title":"Installing Required Packages\u00b6","text":""},{"location":"howto/persist-messages/#use-async-connection","title":"Use Async Connection\u00b6","text":"<p>Note: <code>TableGPT Agent</code> is built based on LangGraph, and many of the <code>Node</code> components use <code>async/await</code> syntax, which does not yet support non-asynchronous operations.</p> <p>Setting up an asynchronous connection to the database allows for non-blocking database operations. This means other parts of your application can continue running while waiting for database operations to complete. This is particularly beneficial in high-concurrency scenarios or when dealing with I/O-bound operations.</p> <p>The <code>DB_URI</code> is the database connection URI, specifying the protocol for connecting to a PostgreSQL database, including authentication and the host where the database is running.</p>"},{"location":"howto/persist-messages/#creating-a-checkpointer-with-asyncpostgressaver","title":"Creating a Checkpointer with AsyncPostgresSaver\u00b6","text":"<p>This creates a connection based on a connection string:</p> <ul> <li>Advantages: Simplicity, encapsulates connection details</li> <li>Best for: Quick setup or when connection details are provided as a string</li> </ul>"},{"location":"howto/persist-messages/#get-persisted-messages-with-config","title":"Get Persisted Messages with Config\u00b6","text":"<p>We can use the same config parameters to retrieve persisted messages through the <code>checkpointer</code>. You can follow the example below:</p>"},{"location":"howto/retrieval/","title":"Enhance TableGPT Agent with RAG","text":"In\u00a0[3]: Copied! <pre>from langchain_core.vectorstores import InMemoryVectorStore\nfrom tablegpt.retriever import CSVLoader\n\nloader = CSVLoader(\"\u4ea7\u54c1\u9500\u91cf\u8868.csv\", autodetect_encoding=True)\n\ndocuments = []\nasync for item in loader.alazy_load():\n    documents.append(item)\n\n# Initialize with an embedding model\nvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())\n\nawait vector_store.aadd_documents(documents=documents)\ndataset_base_retriever = vector_store.as_retriever()\n</pre> from langchain_core.vectorstores import InMemoryVectorStore from tablegpt.retriever import CSVLoader  loader = CSVLoader(\"\u4ea7\u54c1\u9500\u91cf\u8868.csv\", autodetect_encoding=True)  documents = [] async for item in loader.alazy_load():     documents.append(item)  # Initialize with an embedding model vector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())  await vector_store.aadd_documents(documents=documents) dataset_base_retriever = vector_store.as_retriever() In\u00a0[5]: Copied! <pre>from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import DocumentCompressorPipeline\nfrom tablegpt.retriever import ColumnDocCompressor\n\ndataset_compressor = DocumentCompressorPipeline(\n    transformers=[ColumnDocCompressor()]\n)\n\ndataset_retriever = ContextualCompressionRetriever(\n    base_compressor=dataset_compressor,\n    base_retriever=dataset_base_retriever,\n)\n</pre> from langchain.retrievers import ContextualCompressionRetriever from langchain.retrievers.document_compressors import DocumentCompressorPipeline from tablegpt.retriever import ColumnDocCompressor  dataset_compressor = DocumentCompressorPipeline(     transformers=[ColumnDocCompressor()] )  dataset_retriever = ContextualCompressionRetriever(     base_compressor=dataset_compressor,     base_retriever=dataset_base_retriever, ) In\u00a0[8]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom pybox import AsyncLocalPyBoxManager\nfrom tablegpt.agent import create_tablegpt_graph\nfrom tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\npybox_manager = AsyncLocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\n\nagent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    dataset_retriever=dataset_retriever,\n)\n</pre> from langchain_openai import ChatOpenAI from pybox import AsyncLocalPyBoxManager from tablegpt.agent import create_tablegpt_graph from tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") pybox_manager = AsyncLocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)  agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     dataset_retriever=dataset_retriever, ) <p>With this setup, your <code>TableGPT Agent</code> is ready to process user queries, retrieve relevant data, and generate contextually accurate responses. The integration of RAG techniques ensures that the agent leverages external data effectively, providing enhanced insights and performance.</p> In\u00a0[9]: Copied! <pre>from datetime import date\nfrom langchain_core.messages import HumanMessage\n\nmessage = HumanMessage(content=\"\u6843\u9165\u7684\u9500\u552e\u91cf\u662f\u591a\u5c11?\")\n\n_input = {\n    \"messages\": [message],\n    \"parent_id\": \"some-parent-id\",\n    \"date\": date.today(),\n}\n\nresponse = await agent.ainvoke(_input)\n\nresponse[\"messages\"]\n</pre> from datetime import date from langchain_core.messages import HumanMessage  message = HumanMessage(content=\"\u6843\u9165\u7684\u9500\u552e\u91cf\u662f\u591a\u5c11?\")  _input = {     \"messages\": [message],     \"parent_id\": \"some-parent-id\",     \"date\": date.today(), }  response = await agent.ainvoke(_input)  response[\"messages\"] Out[9]: <pre>[HumanMessage(content='\u6843\u9165\u7684\u9500\u552e\u91cf\u662f\u591a\u5c11?', additional_kwargs={}, response_metadata={}, id='b567e1c3-8943-453c-9ebe-fa8d34cfc388'),\n SystemMessage(content='\\nHere are some extra column information that might help you understand the dataset:\\n- \u4ea7\u54c1\u9500\u91cf\u8868.csv:\\n  - {\"column\": \u540d\u79f0, \"dtype\": \"string\", \"values\": [\"\u82b1\u751f\u6843\u9165\", ...]}\\n  - {\"column\":  \u9500\u552e\u989d , \"dtype\": \"string\", \"values\": [\" \uffe5931,000.00 \", \" \uffe5225,060.00 \", \" \uffe558,500.00 \", ...]}\\n', additional_kwargs={'parent_id': 'some-parent-id'}, response_metadata={}, id='07fdddf4-05e8-4022-9a78-98ee3744aab2'),\n AIMessage(content=\"\u4e3a\u4e86\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u9700\u8981\u8bfb\u53d6\u6587\u4ef6`\u4ea7\u54c1\u9500\u91cf\u8868.csv`\uff0c\u7136\u540e\u627e\u5230\u5217\u540d\u5305\u542b\u201c\u540d\u79f0\u201d\u548c\u201c\u9500\u552e\u989d\u201d\u7684\u5217\uff0c\u7279\u522b\u662f\u9700\u8981\u627e\u5230\u201c\u82b1\u751f\u6843\u9165\u201d\u7684\u9500\u552e\u91cf\u3002\u8ba9\u6211\u4eec\u5148\u8bfb\u53d6\u6570\u636e\u5e76\u67e5\u770b\u524d\u51e0\u884c\u3002\\n```python\\nimport pandas as pd\\n\\n# \u8bfb\u53d6\u6570\u636e\\ndf = read_df(uri='\u4ea7\u54c1\u9500\u91cf\u8868.csv')\\n\\n# \u663e\u793a\u6570\u636e\u6846\u7684\u524d\u51e0\u884c\\ndf.head()\\n```\", additional_kwargs={'thought': '\u4e3a\u4e86\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u9700\u8981\u8bfb\u53d6\u6587\u4ef6`\u4ea7\u54c1\u9500\u91cf\u8868.csv`\uff0c\u7136\u540e\u627e\u5230\u5217\u540d\u5305\u542b\u201c\u540d\u79f0\u201d\u548c\u201c\u9500\u552e\u989d\u201d\u7684\u5217\uff0c\u7279\u522b\u662f\u9700\u8981\u627e\u5230\u201c\u82b1\u751f\u6843\u9165\u201d\u7684\u9500\u552e\u91cf\u3002\u8ba9\u6211\u4eec\u5148\u8bfb\u53d6\u6570\u636e\u5e76\u67e5\u770b\u524d\u51e0\u884c\u3002', 'action': {'tool': 'python', 'tool_input': \"import pandas as pd\\n\\n# \u8bfb\u53d6\u6570\u636e\\ndf = read_df(uri='\u4ea7\u54c1\u9500\u91cf\u8868.csv')\\n\\n# \u663e\u793a\u6570\u636e\u6846\u7684\u524d\u51e0\u884c\\ndf.head()\"}, 'parent_id': 'some-parent-id'}, response_metadata={}, id='27da6f10-2201-4349-bc23-9f7b42f34742', tool_calls=[{'name': 'python', 'args': {'query': \"import pandas as pd\\n\\n# \u8bfb\u53d6\u6570\u636e\\ndf = read_df(uri='\u4ea7\u54c1\u9500\u91cf\u8868.csv')\\n\\n# \u663e\u793a\u6570\u636e\u6846\u7684\u524d\u51e0\u884c\\ndf.head()\"}, 'id': 'be9a29de-7f5d-4010-a85b-37286ab99e86', 'type': 'tool_call'}]),\n ToolMessage(content=[{'type': 'text', 'text': '```pycon\\n       \u7f16\u53f7      \u540d\u79f0 \u5355\u4f4d   \u5355\u4ef7\uff08\u5143\uff09      \u9500\u552e\u91cf             \u9500\u552e\u989d \\n0  mb2033    \u6cd5\u5f0f\u9762\u5305  \u5305   \uffe57.40   305080   \uffe52,257,592.00 \\n1  mb2034    \u5976\u6614\u86cb\u7cd5  \u5305   \uffe55.80    93200     \uffe5540,560.00 \\n2  mb2035  \u5976\u6cb9\u5939\u5fc3\u997c\u5e72  \u5305   \uffe53.10   215300     \uffe5667,430.00 \\n3  mb2036     \u8471\u6cb9\u997c  \u5305   \uffe52.20   102300     \uffe5225,060.00 \\n4  mb2037    \u82b1\u751f\u6843\u9165  \u5305   \uffe53.80   130000     \uffe5494,000.00 \\n```'}], name='python', id='a48d70fd-2e01-48ee-a9a5-25dc0eec04d6', tool_call_id='be9a29de-7f5d-4010-a85b-37286ab99e86', artifact=[]),\n AIMessage(content='\u4ece\u6570\u636e\u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u201c\u82b1\u751f\u6843\u9165\u201d\u7684\u9500\u552e\u91cf\u4e3a130,000\u5305\u3002', additional_kwargs={'parent_id': 'some-parent-id'}, response_metadata={}, id='5c5b703d-2eea-444b-a627-0828dca06df2')]</pre> <p>Output:</p> <p>Here are some extra column information that might help you understand the dataset:</p> <ul> <li>\u4ea7\u54c1\u9500\u91cf\u8868.csv:</li> <li>{\"column\": \u540d\u79f0, \"dtype\": \"string\", \"values\": [\"\u82b1\u751f\u6843\u9165\", ...]}</li> <li>{\"column\":  \u9500\u552e\u989d , \"dtype\": \"string\", \"values\": [\" \uffe5931,000.00 \", \" \uffe5225,060.00 \", \" \uffe558,500.00 \", ...]}</li> </ul> <p>The output confirms that the RAG approach effectively enriches the agent's responses by incorporating dataset context. This improvement allows the agent to provide detailed, actionable insights rather than generic answers, thereby enhancing its utility for complex queries.</p>"},{"location":"howto/retrieval/#enhance-tablegpt-agent-with-rag","title":"Enhance TableGPT Agent with RAG\u00b6","text":"<p>While the File Reading Workflow is adequate for most scenarios, it may not always provide the information necessary for the LLM to generate accurate code. Consider the following examples:</p> <ul> <li>A categorical column in the dataset contains 'foo', 'bar', and 'baz', but 'baz' only appears after approximately 100 rows. In this case, the LLM may not encounter the 'baz' value through <code>df.head()</code>.</li> <li>The user's query may not align with the dataset's content for several reasons:<ul> <li>The dataset lacks proper governance. For instance, a cell value might be misspelled from 'foo' to 'fou'.</li> <li>There could be a typo in the user's query. For example, if the user queries, \"Show me the data for 'fou',\" but the dataset contains 'foo' instead.</li> </ul> </li> </ul> <p>In such situations, the Dataset Retriever plugin can be utilized to fetch additional information about the dataset from external sources, thereby providing the LLM with more context and improving its ability to generate accurate responses.</p>"},{"location":"howto/retrieval/#quick-start","title":"Quick Start\u00b6","text":"<p>To help you quickly integrate and utilize <code>RAG</code> with the <code>TableGPT Agent</code>, follow the steps outlined in this section. These instructions will guide you through the process of loading datasets, enhancing retrieval with document compression, and integrating with a powerful LLM-based agent. By the end of this quick start, you'll be able to issue complex queries and receive enriched, context-aware responses.</p>"},{"location":"howto/retrieval/#step-1-install-required-dependencies","title":"Step 1: Install Required Dependencies\u00b6","text":"<p>To get started with using RAG in the TableGPT Agent, you need to install the necessary dependencies. The primary package required is langchain, which facilitates building retrieval-augmented workflows.</p> <p>Run the following command to install it:</p> <pre>pip install langchain\n</pre>"},{"location":"howto/retrieval/#step-2-load-and-prepare-data-with-csvloader","title":"Step 2: Load and Prepare Data with CSVLoader\u00b6","text":"<p>The <code>TableGPT Agent</code> provides a convenient <code>CSVLoader</code> for converting <code>CSV</code> or <code>Excel</code> files into a format that can be processed by the RAG pipeline. This method allows seamless integration of your data for further retrieval and embedding.</p> <p>Example Code:</p>"},{"location":"howto/retrieval/#step-3-build-a-context-aware-retriever-with-document-compression","title":"Step 3: Build a Context-Aware Retriever with Document Compression\u00b6","text":"<p>To enhance the retrieval process, <code>langchain</code> provides powerful retriever utilities that can be combined with custom compressors. In this step, we utilize the <code>ColumnDocCompressor</code> from tablegpt to focus on relevant columns and build an efficient <code>dataset_retriever</code>.</p> <p>Example Code:</p>"},{"location":"howto/retrieval/#step-4-integrate-with-tablegpt-agent","title":"Step 4: Integrate with TableGPT Agent\u00b6","text":"<p>In this step, we integrate the <code>dataset_retriever</code> with the <code>TableGPT Agent</code> using an <code>LLM</code> and a local execution environment. This setup ensures that the agent can handle user queries effectively by leveraging both the LLM and retrieved dataset context.</p> <p>Example Code:</p>"},{"location":"howto/retrieval/#step-5-analyze-data-with-the-tablegpt-agent","title":"Step 5: Analyze Data with the TableGPT Agent\u00b6","text":"<p>Finally, you can use the <code>TableGPT Agent</code> to perform analysis by sending a query. The response can help determine whether retrieval-augmented generation (RAG) has provided enhanced results. Observing the returned information allows you to assess the accuracy and completeness of the generated response.</p> <p>Example Code:</p>"},{"location":"tutorials/chat-on-tabular-data/","title":"Chat on Tabular Data","text":"In\u00a0[1]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom pybox import AsyncLocalPyBoxManager\nfrom tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\nfrom tablegpt.agent import create_tablegpt_graph\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\npybox_manager = AsyncLocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\ncheckpointer = MemorySaver()\n\nagent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    checkpointer=checkpointer,\n    session_id=\"some-session-id\", # This is required when using file-reading\n)\n</pre> from langchain_openai import ChatOpenAI from langgraph.checkpoint.memory import MemorySaver from pybox import AsyncLocalPyBoxManager from tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR from tablegpt.agent import create_tablegpt_graph  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") pybox_manager = AsyncLocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR) checkpointer = MemorySaver()  agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     checkpointer=checkpointer,     session_id=\"some-session-id\", # This is required when using file-reading ) <p>Add the file for processing in the additional_kwargs of HumanMessage. Here's an example using the Titanic dataset.</p> In\u00a0[2]: Copied! <pre>from typing import TypedDict\nfrom langchain_core.messages import HumanMessage\n\nclass Attachment(TypedDict):\n    \"\"\"Contains at least one dictionary with the key filename.\"\"\"\n    filename: str\n\nattachment_msg = HumanMessage(\n    content=\"\",\n    # Please make sure your iPython kernel can access your filename.\n    additional_kwargs={\"attachments\": [Attachment(filename=\"titanic.csv\")]},\n)\n</pre> from typing import TypedDict from langchain_core.messages import HumanMessage  class Attachment(TypedDict):     \"\"\"Contains at least one dictionary with the key filename.\"\"\"     filename: str  attachment_msg = HumanMessage(     content=\"\",     # Please make sure your iPython kernel can access your filename.     additional_kwargs={\"attachments\": [Attachment(filename=\"titanic.csv\")]}, ) <p>Invoke the agent as shown in the quick start:</p> In\u00a0[3]: Copied! <pre>from datetime import date\nfrom tablegpt.agent.file_reading import Stage\n\n# Reading and processing files.\nresponse = await agent.ainvoke(\n    input={\n        \"entry_message\": attachment_msg,\n        \"processing_stage\": Stage.UPLOADED,\n        \"messages\": [attachment_msg],\n        \"parent_id\": \"some-parent-id1\",\n        \"date\": date.today(),\n    },\n    config={\n        # Using checkpointer requires binding thread_id at runtime.\n        \"configurable\": {\"thread_id\": \"some-thread-id\"},\n    },\n)\nresponse[\"messages\"]\n</pre> from datetime import date from tablegpt.agent.file_reading import Stage  # Reading and processing files. response = await agent.ainvoke(     input={         \"entry_message\": attachment_msg,         \"processing_stage\": Stage.UPLOADED,         \"messages\": [attachment_msg],         \"parent_id\": \"some-parent-id1\",         \"date\": date.today(),     },     config={         # Using checkpointer requires binding thread_id at runtime.         \"configurable\": {\"thread_id\": \"some-thread-id\"},     }, ) response[\"messages\"] Out[3]: <pre>[HumanMessage(content='', additional_kwargs={'attachments': [{'filename': 'titanic.csv'}]}, response_metadata={}, id='ab0a7157-ad7d-4de8-9b24-1bee78ad7c55'),\n AIMessage(content=\"\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u6570\u636e\u6587\u4ef6\uff0c\u6211\u9700\u8981\u67e5\u770b\u6587\u4ef6\u5185\u5bb9\u4ee5\u5bf9\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u521d\u6b65\u7684\u4e86\u89e3\u3002\u9996\u5148\u6211\u4f1a\u8bfb\u53d6\u6570\u636e\u5230 `df` \u53d8\u91cf\u4e2d\uff0c\u5e76\u901a\u8fc7 `df.info` \u67e5\u770b NaN \u60c5\u51b5\u548c\u6570\u636e\u7c7b\u578b\u3002\\n```python\\n# Load the data into a DataFrame\\ndf = read_df('titanic.csv')\\n\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how='all').dropna(axis=1, how='all')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\\n```\", additional_kwargs={'parent_id': 'some-parent-id1', 'thought': '\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u6570\u636e\u6587\u4ef6\uff0c\u6211\u9700\u8981\u67e5\u770b\u6587\u4ef6\u5185\u5bb9\u4ee5\u5bf9\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u521d\u6b65\u7684\u4e86\u89e3\u3002\u9996\u5148\u6211\u4f1a\u8bfb\u53d6\u6570\u636e\u5230 `df` \u53d8\u91cf\u4e2d\uff0c\u5e76\u901a\u8fc7 `df.info` \u67e5\u770b NaN \u60c5\u51b5\u548c\u6570\u636e\u7c7b\u578b\u3002', 'action': {'tool': 'python', 'tool_input': \"# Load the data into a DataFrame\\ndf = read_df('titanic.csv')\\n\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how='all').dropna(axis=1, how='all')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\"}, 'model_type': None}, response_metadata={}, id='add6691d-d7ea-411d-9699-e99ae0b7de97', tool_calls=[{'name': 'python', 'args': {'query': \"# Load the data into a DataFrame\\ndf = read_df('titanic.csv')\\n\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how='all').dropna(axis=1, how='all')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\"}, 'id': 'b846aa01-04ef-4669-9a5c-53ddcb9a2dfb', 'type': 'tool_call'}]),\n ToolMessage(content=[{'type': 'text', 'text': \"```pycon\\n&lt;class 'pandas.core.frame.DataFrame'&gt;\\nRangeIndex: 4 entries, 0 to 3\\nData columns (total 8 columns):\\n #   Column    Non-Null Count  Dtype  \\n---  ------    --------------  -----  \\n 0   Pclass    4 non-null      int64  \\n 1   Sex       4 non-null      object \\n 2   Age       4 non-null      float64\\n 3   SibSp     4 non-null      int64  \\n 4   Parch     4 non-null      int64  \\n 5   Fare      4 non-null      float64\\n 6   Embarked  4 non-null      object \\n 7   Survived  4 non-null      int64  \\ndtypes: float64(2), int64(4), object(2)\\n```\"}], name='python', id='0d441b21-bff3-463c-a07f-c0b12bd17bc5', tool_call_id='b846aa01-04ef-4669-9a5c-53ddcb9a2dfb', artifact=[]),\n AIMessage(content='\u63a5\u4e0b\u6765\u6211\u5c06\u7528 `df.head(5)` \u6765\u67e5\u770b\u6570\u636e\u96c6\u7684\u524d 5 \u884c\u3002\\n```python\\n# Show the first 5 rows to understand the structure\\ndf.head(5)\\n```', additional_kwargs={'parent_id': 'some-parent-id1', 'thought': '\u63a5\u4e0b\u6765\u6211\u5c06\u7528 `df.head(5)` \u6765\u67e5\u770b\u6570\u636e\u96c6\u7684\u524d 5 \u884c\u3002', 'action': {'tool': 'python', 'tool_input': '# Show the first 5 rows to understand the structure\\ndf.head(5)'}, 'model_type': None}, response_metadata={}, id='5e26ef1d-7042-471e-b39f-194a51a185c7', tool_calls=[{'name': 'python', 'args': {'query': '# Show the first 5 rows to understand the structure\\ndf.head(5)'}, 'id': 'f6be0d96-05b3-4b5b-8313-90197a8c3d87', 'type': 'tool_call'}]),\n ToolMessage(content=[{'type': 'text', 'text': '```pycon\\n   Pclass     Sex   Age  SibSp  Parch    Fare Embarked  Survived\\n0       2  female  29.0      0      2  23.000        S         1\\n1       3  female  39.0      1      5  31.275        S         0\\n2       3    male  26.5      0      0   7.225        C         0\\n3       3    male  32.0      0      0  56.496        S         1\\n```'}], name='python', id='6fc6d8aa-546c-467e-91d3-d57b0b62dd68', tool_call_id='f6be0d96-05b3-4b5b-8313-90197a8c3d87', artifact=[]),\n AIMessage(content='\u6211\u5df2\u7ecf\u4e86\u89e3\u4e86\u6570\u636e\u96c6 titanic.csv \u7684\u57fa\u672c\u4fe1\u606f\u3002\u8bf7\u95ee\u6211\u53ef\u4ee5\u5e2e\u60a8\u505a\u4e9b\u4ec0\u4e48\uff1f', additional_kwargs={'parent_id': 'some-parent-id1'}, response_metadata={}, id='b6dc3885-94cb-4b0f-b691-f37c4c8c9ba3')]</pre> <p>Continue to ask questions for data analysis:</p> In\u00a0[4]: Copied! <pre>human_message = HumanMessage(content=\"How many men survived?\")\n\nasync for event in agent.astream_events(\n    input={\n        # After using checkpoint, you only need to add new messages here.\n        \"messages\": [human_message],\n        \"parent_id\": \"some-parent-id2\",\n        \"date\": date.today(),\n    },\n    version=\"v2\",\n    # We configure the same thread_id to use checkpoints to retrieve the memory of the last run.\n    config={\"configurable\": {\"thread_id\": \"some-thread-id\"}},\n):\n    event_name: str = event[\"name\"]\n    evt: str = event[\"event\"]\n    if evt == \"on_chat_model_end\":\n        print(event[\"data\"][\"output\"])\n    elif event_name == \"tool_node\" and evt == \"on_chain_stream\":\n        for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:\n            print(lc_msg)\n    else:\n        # Other events can be handled here.\n        pass\n</pre> human_message = HumanMessage(content=\"How many men survived?\")  async for event in agent.astream_events(     input={         # After using checkpoint, you only need to add new messages here.         \"messages\": [human_message],         \"parent_id\": \"some-parent-id2\",         \"date\": date.today(),     },     version=\"v2\",     # We configure the same thread_id to use checkpoints to retrieve the memory of the last run.     config={\"configurable\": {\"thread_id\": \"some-thread-id\"}}, ):     event_name: str = event[\"name\"]     evt: str = event[\"event\"]     if evt == \"on_chat_model_end\":         print(event[\"data\"][\"output\"])     elif event_name == \"tool_node\" and evt == \"on_chain_stream\":         for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:             print(lc_msg)     else:         # Other events can be handled here.         pass  <pre>content=\"\u4e3a\u4e86\u56de\u7b54\u60a8\u7684\u95ee\u9898\uff0c\u6211\u5c06\u7b5b\u9009\u51fa\u6240\u6709\u7537\u6027\u4e58\u5ba2\u5e76\u8ba1\u7b97\u5176\u4e2d\u7684\u5e78\u5b58\u8005\u6570\u91cf\u3002\\n```python\\n# Filter male passengers who survived and count them\\nmale_survivors = df[(df['Sex'] == 'male') &amp; (df['Survived'] == 1)]\\nmale_survivors_count = male_survivors.shape[0]\\nmale_survivors_count\\n```\" additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-661d7496-341d-4a6b-84d8-b4094db66ef0'\ncontent=[{'type': 'text', 'text': '```pycon\\n1\\n```'}] name='python' id='1c7531db-9150-451d-a8dd-f07176454e6f' tool_call_id='2860e8bb-0fa7-421b-bb2d-bfeca873354b' artifact=[]\ncontent='\u6839\u636e\u6570\u636e\u96c6\uff0c\u6709 1 \u540d\u7537\u6027\u4e58\u5ba2\u5e78\u5b58\u3002' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-db640705-0085-4f47-adb4-3e0adce694cd'\n</pre>"},{"location":"tutorials/chat-on-tabular-data/#chat-on-tabular-data","title":"Chat on Tabular Data\u00b6","text":"<p>TableGPT Agent excels at analyzing and processing tabular data. To perform data analysis, you need to first let the agent \"see\" the dataset. This is done by a specific \"file-reading\" workflow. In short, you begin by \"uploading\" the dataset and let the agent read it. Once the data is read, you can ask the agent questions about it.</p> <p>To learn more about the file-reading workflow, see File Reading.</p> <p>For data analysis tasks, we introduce two important parameters when creating the agent: <code>checkpointer</code> and <code>session_id</code>.</p> <ul> <li>The <code>checkpointer</code> should be an instance of <code>langgraph.checkpoint.base.BaseCheckpointSaver</code>, which acts as a versioned \"memory\" for the agent. (See langgraph's persistence concept for more details.)</li> <li>The <code>session_id</code> is a unique identifier for the current session. It ties the agent's execution to a specific kernel, ensuring that the agent's results are retained across multiple invocations.</li> </ul>"},{"location":"tutorials/continue-analysis-on-generated-charts/","title":"Continue Analysis on Generated Charts","text":"In\u00a0[1]: Copied! <pre>from datetime import date\nfrom typing import TypedDict\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom pybox import AsyncLocalPyBoxManager\nfrom tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\nfrom tablegpt.agent import create_tablegpt_graph\nfrom tablegpt.agent.file_reading import Stage\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\npybox_manager = AsyncLocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\ncheckpointer = MemorySaver()\n\nagent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    checkpointer=checkpointer,\n    session_id=\"some-session-id\", # This is required when using file-reading\n)\n\nclass Attachment(TypedDict):\n    \"\"\"Contains at least one dictionary with the key filename.\"\"\"\n    filename: str\n\nattachment_msg = HumanMessage(\n    content=\"\",\n    # Please make sure your iPython kernel can access your filename.\n    additional_kwargs={\"attachments\": [Attachment(filename=\"titanic.csv\")]},\n)\n\n# Reading and processing files.\nresponse = await agent.ainvoke(\n    input={\n        \"entry_message\": attachment_msg,\n        \"processing_stage\": Stage.UPLOADED,\n        \"messages\": [attachment_msg],\n        \"parent_id\": \"some-parent-id1\",\n        \"date\": date.today(),\n    },\n    config={\n        # Using checkpointer requires binding thread_id at runtime.\n        \"configurable\": {\"thread_id\": \"some-thread-id\"},\n    },\n)\n</pre> from datetime import date from typing import TypedDict  from langchain_core.messages import HumanMessage from langchain_openai import ChatOpenAI from langgraph.checkpoint.memory import MemorySaver from pybox import AsyncLocalPyBoxManager from tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR from tablegpt.agent import create_tablegpt_graph from tablegpt.agent.file_reading import Stage  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") pybox_manager = AsyncLocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR) checkpointer = MemorySaver()  agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     checkpointer=checkpointer,     session_id=\"some-session-id\", # This is required when using file-reading )  class Attachment(TypedDict):     \"\"\"Contains at least one dictionary with the key filename.\"\"\"     filename: str  attachment_msg = HumanMessage(     content=\"\",     # Please make sure your iPython kernel can access your filename.     additional_kwargs={\"attachments\": [Attachment(filename=\"titanic.csv\")]}, )  # Reading and processing files. response = await agent.ainvoke(     input={         \"entry_message\": attachment_msg,         \"processing_stage\": Stage.UPLOADED,         \"messages\": [attachment_msg],         \"parent_id\": \"some-parent-id1\",         \"date\": date.today(),     },     config={         # Using checkpointer requires binding thread_id at runtime.         \"configurable\": {\"thread_id\": \"some-thread-id\"},     }, ) In\u00a0[2]: Copied! <pre># Define the human message that asks the model to draw a pie chart based on gender data\nhuman_message = HumanMessage(content=\"Draw a pie chart based on gender and the number of people of each gender.\")\n\nasync for event in agent.astream_events(\n    input={\n        \"messages\": [human_message],\n        \"parent_id\": \"some-parent-id2\",\n        \"date\": date.today(),\n    },\n    version=\"v2\",\n    # We configure the same thread_id to use checkpoints to retrieve the memory of the last run.\n    config={\"configurable\": {\"thread_id\": \"some-thread-id\"}},\n):\n    evt = event[\"event\"]\n    if evt == \"on_chat_model_end\":\n        print(event[\"data\"][\"output\"])\n    elif event[\"name\"] == \"tool_node\" and evt == \"on_chain_stream\":\n        for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:\n            print(lc_msg)\n    else:\n        # Handle other events here\n        pass\n</pre> # Define the human message that asks the model to draw a pie chart based on gender data human_message = HumanMessage(content=\"Draw a pie chart based on gender and the number of people of each gender.\")  async for event in agent.astream_events(     input={         \"messages\": [human_message],         \"parent_id\": \"some-parent-id2\",         \"date\": date.today(),     },     version=\"v2\",     # We configure the same thread_id to use checkpoints to retrieve the memory of the last run.     config={\"configurable\": {\"thread_id\": \"some-thread-id\"}}, ):     evt = event[\"event\"]     if evt == \"on_chat_model_end\":         print(event[\"data\"][\"output\"])     elif event[\"name\"] == \"tool_node\" and evt == \"on_chain_stream\":         for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:             print(lc_msg)     else:         # Handle other events here         pass <pre>content=\"\u597d\u7684\uff0c\u6211\u5c06\u57fa\u4e8e\u6027\u522b\u7ed8\u5236\u4e00\u4e2a\u997c\u56fe\uff0c\u4ee5\u5c55\u793a\u6bcf\u4e2a\u6027\u522b\u7684\u4eba\u6570\u3002\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u7edf\u8ba1\u6bcf\u4e2a\u6027\u522b\u7684\u4eba\u6570\uff0c\u7136\u540e\u4f7f\u7528 `seaborn` \u548c `matplotlib` \u6765\u7ed8\u5236\u997c\u56fe\u3002\\n\\n```python\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Count the number of people for each gender\\ngender_counts = df['Sex'].value_counts()\\n\\n# Create a pie chart\\nplt.figure(figsize=(8, 6))\\nplt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))\\nplt.title('Gender Distribution')\\nplt.show()\\n```\" additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-6115fe22-3b55-4d85-be09-6c31a59736f6'\ncontent=[{'type': 'text', 'text': '```pycon\\n&lt;Figure size 800x600 with 1 Axes&gt;\\n```'}, {'type': 'image_url', 'image_url': {'url': 'data:image/png;base64,iVBORw0KG...'}}] name='python' id='226ba8f2-29a7-4706-9178-8cb5b4062488' tool_call_id='03eb1113-6aed-4e0a-a3c0-4cc0043a55ee' artifact=[]\ncontent='\u997c\u56fe\u5df2\u7ecf\u6210\u529f\u751f\u6210\u3002' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-83468bd1-9451-4c78-91a3-b0f96ffa169a'\n</pre> <p>Now let's set up the Visual Language Model (VLM) and create a new agent with VLM support:</p> In\u00a0[3]: Copied! <pre># Initialize the VLM instance\nvlm = ChatOpenAI(openai_api_base=\"YOUR_VLM_URL\", openai_api_key=\"whatever\", model_name=\"YOUR_MODEL_NAME\")\n\n# Assume llm, pybox_manager, and memory_saver are defined elsewhere\nagent_with_vlm = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    vlm=vlm,\n    checkpointer=checkpointer,\n    session_id=\"some-session-id\",\n)\n</pre> # Initialize the VLM instance vlm = ChatOpenAI(openai_api_base=\"YOUR_VLM_URL\", openai_api_key=\"whatever\", model_name=\"YOUR_MODEL_NAME\")  # Assume llm, pybox_manager, and memory_saver are defined elsewhere agent_with_vlm = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     vlm=vlm,     checkpointer=checkpointer,     session_id=\"some-session-id\", ) <p>We use a time travel feature to go back to before the last time the agent gave an answer, to avoid past memories hallucinating the model:</p> In\u00a0[4]: Copied! <pre>state_history = agent.get_state_history(config={\"configurable\": {\"thread_id\": \"some-thread-id\"}})\n\nto_replay = None\nfor state in list(state_history)[::-1]:\n    if state.next and state.next[0] == \"__start__\":\n        to_replay = state\n</pre> state_history = agent.get_state_history(config={\"configurable\": {\"thread_id\": \"some-thread-id\"}})  to_replay = None for state in list(state_history)[::-1]:     if state.next and state.next[0] == \"__start__\":         to_replay = state <p>Send the same question to the model via the new agent with VLM support</p> In\u00a0[5]: Copied! <pre>async for event in agent_with_vlm.astream_events(\n    None,\n    to_replay.config,\n    version=\"v2\",\n):\n    evt = event[\"event\"]\n    if evt == \"on_chat_model_end\":\n        print(event[\"data\"][\"output\"])\n    elif event[\"name\"] == \"tool_node\" and evt == \"on_chain_stream\":\n        for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:\n            print(lc_msg)\n    else:\n        # Handle other events here\n        pass\n</pre> async for event in agent_with_vlm.astream_events(     None,     to_replay.config,     version=\"v2\", ):     evt = event[\"event\"]     if evt == \"on_chat_model_end\":         print(event[\"data\"][\"output\"])     elif event[\"name\"] == \"tool_node\" and evt == \"on_chain_stream\":         for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:             print(lc_msg)     else:         # Handle other events here         pass <pre>content=\"\u597d\u7684\uff0c\u6211\u5c06\u7ed8\u5236\u4e00\u4e2a\u997c\u56fe\u6765\u5c55\u793a\u6570\u636e\u96c6\u4e2d\u7537\u6027\u548c\u5973\u6027\u4e58\u5ba2\u7684\u6570\u91cf\u3002\\n```python\\n# Count the number of passengers by gender\\ngender_counts = df['Sex'].value_counts()\\n\\n# Plot a pie chart\\nplt.figure(figsize=(8, 6))\\nplt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=140)\\nplt.title('Gender Distribution')\\nplt.show()\\n```\\n\" additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-2d05b2ab-32f4-481f-8fa5-43c78515d9c3'\ncontent=[{'type': 'text', 'text': '```pycon\\n&lt;Figure size 800x600 with 1 Axes&gt;\\n```'}, {'type': 'image_url', 'image_url': {'url': 'data:image/png;base64,iVBORw0K...'}}] name='python' id='51a99935-b0b1-496d-9a45-c1f318104773' tool_call_id='918d57ee-7362-4e0d-8d66-64b7e57ecaf6' artifact=[]\ncontent='\u997c\u56fe\u663e\u793a\u6570\u636e\u96c6\u4e2d\u6027\u522b\u5206\u5e03\u4e3a 50% \u5973\u6027\u548c 50% \u7537\u6027\uff0c\u8fd9\u8868\u660e\u7537\u6027\u548c\u5973\u6027\u4e58\u5ba2\u6570\u91cf\u76f8\u7b49\u3002' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'qwen2-vl-7b-instruct'} id='run-d9b0e891-f03c-40c8-8474-9fef7511c40b'\n</pre> <p>We observe that the answer provided by the agent with VLM support is significantly more detailed, including a comprehensive description of the generated images.</p>"},{"location":"tutorials/continue-analysis-on-generated-charts/#continue-analysis-on-generated-charts","title":"Continue Analysis on Generated Charts\u00b6","text":"<p>While TableGPT2 excels in data analysis tasks, it currently lacks built-in support for visual modalities. Many data analysis tasks involve visualization, so to address this limitation, we provide an interface for integrating your own Visual Language Model (VLM) plugin.</p> <p>When the agent performs a visualization task\u2014typically using <code>matplotlib.pyplot.show</code>\u2014the VLM will take over from the LLM, offering a more nuanced summarization of the visualization. This approach avoids the common pitfalls of LLMs in visualization tasks, which often either state, \"I have plotted the data,\" or hallucinating the content of the plot.</p> <p>We continue using the agent from the previous section to perform a data visualization task and observe its final output.</p> <p>NOTE Before you start, you can install Chinese fonts using the following command:</p> <pre>apt-get update &amp;&amp; apt-get install -y --no-install-recommends fonts-noto-cjk\nmplfonts init\n</pre>"},{"location":"tutorials/quick-start/","title":"Quickstart","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install tablegpt-agent\n</pre> %pip install tablegpt-agent <p>TableGPT Agent depends on pybox to manage code execution environment. By default, pybox operates in an in-cluster mode. If you intend to run tablegpt-agent in a local environment, install the optional dependency as follows:</p> In\u00a0[1]: Copied! <pre>%pip install tablegpt-agent[local]\n</pre> %pip install tablegpt-agent[local] <p>This tutorial uses <code>langchain-openai</code> for the chat model instance. Please make sure you have it installed:</p> In\u00a0[\u00a0]: Copied! <pre>%pip install langchain-openai\n</pre> %pip install langchain-openai In\u00a0[\u00a0]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom pybox import AsyncLocalPyBoxManager\nfrom tablegpt.agent import create_tablegpt_graph\n\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\npybox_manager = AsyncLocalPyBoxManager()\n\nagent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n)\n</pre> from langchain_openai import ChatOpenAI from pybox import AsyncLocalPyBoxManager from tablegpt.agent import create_tablegpt_graph   llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") pybox_manager = AsyncLocalPyBoxManager()  agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager, ) In\u00a0[3]: Copied! <pre>from datetime import date\nfrom langchain_core.messages import HumanMessage\n\nmessage = HumanMessage(content=\"Hi\")\n\n_input = {\n    \"messages\": [message],\n    \"parent_id\": \"some-parent-id\",\n    \"date\": date.today(),\n}\n\nstate = await agent.ainvoke(_input)\nstate[\"messages\"]\n</pre> from datetime import date from langchain_core.messages import HumanMessage  message = HumanMessage(content=\"Hi\")  _input = {     \"messages\": [message],     \"parent_id\": \"some-parent-id\",     \"date\": date.today(), }  state = await agent.ainvoke(_input) state[\"messages\"] <pre>[HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}, id='34fe748c-81ab-49ea-bec6-9c621598a61a'), AIMessage(content=\"Hello! How can I assist you with data analysis today? Please let me know the details of the dataset you're working with and what specific analysis you'd like to perform.\", additional_kwargs={'parent_id': 'some-parent-id'}, response_metadata={}, id='a1ee29d2-723e-41c7-b420-27d0cfaed5dc')]\n</pre> <p>You can get more detailed outputs with the <code>astream_events</code> method:</p> In\u00a0[4]: Copied! <pre>async for event in agent.astream_events(\n    input=_input,\n    version=\"v2\",\n):\n    # We ignore irrelevant events here.\n    if event[\"event\"] == \"on_chat_model_end\":\n        print(event[\"data\"][\"output\"])\n</pre> async for event in agent.astream_events(     input=_input,     version=\"v2\", ):     # We ignore irrelevant events here.     if event[\"event\"] == \"on_chat_model_end\":         print(event[\"data\"][\"output\"]) <pre>content='Hello! How can I assist you with your data analysis today? Please let me know what dataset you are working with and what specific analyses or visualizations you would like to perform.' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-525eb149-0e3f-4b04-868b-708295f789ac'\n</pre>"},{"location":"tutorials/quick-start/#quickstart","title":"Quickstart\u00b6","text":""},{"location":"tutorials/quick-start/#installation","title":"Installation\u00b6","text":"<p>To install TableGPT Agent, use the following command:</p>"},{"location":"tutorials/quick-start/#setup-the-llm-service","title":"Setup the LLM Service\u00b6","text":"<p>Before using TableGPT Agent, ensure you have an OpenAI-compatible server configured to host TableGPT2. We recommend using vllm for this:</p> <pre>python -m vllm.entrypoints.openai.api_server --served-model-name TableGPT2-7B --model path/to/weights\n</pre> <p>NOTES:</p> <ul> <li>To analyze tabular data with <code>tablegpt-agent</code>, make sure <code>TableGPT2</code> is served with <code>vllm</code> version 0.5.5 or higher.</li> <li>For production environments, it's important to optimize the vllm server configuration. For details, refer to the vllm documentation on server configuration.</li> </ul>"},{"location":"tutorials/quick-start/#create-tablegpt-agent","title":"Create TableGPT Agent\u00b6","text":"<p>NOTE: TableGPT Agent fully supports aync invocation. If you are running this tutorial in a Jupyter Notebook, no additional setup is required. However, if you plan to run the tutorial in a Python console, make sure to use a console that supports asynchronous operations. To get started, execute the following command:</p> <pre>python -m asyncio\n</pre> <p>In the console or notebook, create the agent as follows:</p>"},{"location":"tutorials/quick-start/#start-chatting","title":"Start Chatting\u00b6","text":""}]}